# Copyright 2025 The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import copy
from typing import List, Optional

import torch
from torch import nn
from transformers import (
    AutoConfig,
    AutoModel,
    AutoModelForImageTextToText,
    AutoProcessor,
    SmolVLMForConditionalGeneration,
)
from peft import LoraConfig, get_peft_model

def apply_rope(x, positions, max_wavelength=10_000):
    """
    Applies RoPE positions [B, L] to x [B, L, H, D].
    """
    d_half = x.shape[-1] // 2
    device = x.device
    dtype = x.dtype
    x = x.to(torch.float32)

    freq_exponents = (2.0 / x.shape[-1]) * torch.arange(d_half, dtype=torch.float32, device=device)
    timescale = max_wavelength**freq_exponents
    radians = positions[..., None].to(torch.float32) / timescale[None, None, :].to(torch.float32)

    radians = radians[..., None, :]

    sin = torch.sin(radians)  # .to(dtype=dtype)
    cos = torch.cos(radians)  # .to(dtype=dtype)

    x1, x2 = x.split(d_half, dim=-1)
    res = torch.empty_like(x)
    res[..., :d_half] = x1 * cos - x2 * sin
    res[..., d_half:] = x2 * cos + x1 * sin

    return res.to(dtype)


def get_intermediate_size(hidden_dim, ffn_dim_multiplier=4, multiple_of=256):
    """
    Function:
        ç”¨äºè®¡ç®— Transformer æ¨¡å‹ä¸­å‰é¦ˆç¥ç»ç½‘ç»œ (FFN) å±‚çš„ä¸­é—´ç»´åº¦ (intermediate size)
        è¯¥å‡½æ•°é¦–å…ˆå°†è¾“å…¥ç»´åº¦ç¼©å°ä¸ºåŸæ¥çš„ 2/3, å†ä¹˜ä»¥ä¸€ä¸ªæ‰©å±•å€æ•° (å¦‚ 4) æœ€åå°†ç»“æœå‘ä¸Šå–æ•´ä¸º
        `multiple_of` çš„å€æ•°, ä»¥æå‡åœ¨ç¡¬ä»¶ (GPU) ä¸Šçš„æ‰§è¡Œæ•ˆç‡

    Args:
        hidden_dim (int): 
            è¾“å…¥çš„éšè—ç»´åº¦ï¼Œé€šå¸¸æ˜¯ Transformer ä¸­çš„ embedding size
        ffn_dim_multiplier (int, optional): 
            ç”¨äºæ‰©å±•éšè—ç»´åº¦çš„ä¹˜æ³•ç³»æ•°ã€‚é»˜è®¤ä¸º 4
        multiple_of (int, optional): 
            è¾“å‡ºç»´åº¦å°†è¢«å‘ä¸Šå–æ•´ä¸ºè¯¥å€¼çš„å€æ•°, ç›®çš„æ˜¯å¯¹é½ç¡¬ä»¶ç‰¹æ€§ä»¥æé«˜è®¡ç®—æ€§èƒ½, é»˜è®¤ä¸º 256

    Returns:
        int: è®¡ç®—åçš„ä¸­é—´å±‚ç»´åº¦, å¯ç”¨äºæ„å»º FFN æ¨¡å—

    Example:
        >>> get_intermediate_size(480)
        1280
    """
    # step 1 : å¯¹è¾“å…¥ç»´åº¦åšä¸€æ¬¡å‹ç¼©                  hidden_dim = int(2 * 480 / 3) = 320
    hidden_dim = int(2 * hidden_dim / 3)
    # step 2 : å°†å‹ç¼©åçš„ç»´åº¦æŒ‰ä¸€å®šå€æ•°æ‰©å±•,          hidden_dim = int(4 * 320) = 1280
    hidden_dim = int(ffn_dim_multiplier * hidden_dim)
    # step 3 : å‘ä¸Šå–æ•´åˆ° nearest multiple çš„å€æ•°,  hidden_dim = 1280
    hidden_dim = multiple_of * ((hidden_dim + multiple_of - 1) // multiple_of)
    return hidden_dim


class SmolVLMWithExpertModel(nn.Module):
    def __init__(
        self,
        model_id: str = "HuggingFaceTB/SmolVLM2-500M-Video-Instruct",
        load_vlm_weights: bool = True,
        train_expert_only: bool = False,
        freeze_vision_encoder: bool = False,
        attention_mode: str = "self_attn",
        num_expert_layers: int = -1,
        num_vlm_layers: int = -1,
        self_attn_every_n_layers: int = -1,
        expert_width_multiplier: float = 0.5,
        train_smolvlm_only: bool = False,
        lora_smolvlm:bool = False,    # æ˜¯å¦ä½¿ç”¨ LoRA
        lora_r:int  = 8,
        lora_alpha:float= 16,
        lora_dropout:float= 0.05,
        laq_code_seq_length:int=8, 
        laq_latent_dim:int=1024,
        chunk_size:int=50,

    ):
        super().__init__()
        if load_vlm_weights:
            print(f"Loading  {model_id} weights ...")
            self.vlm = AutoModelForImageTextToText.from_pretrained(
                model_id,
                device_map="auto",
                torch_dtype="bfloat16",
                low_cpu_mem_usage=True,
            )
            config = self.vlm.config
        else:
            config = AutoConfig.from_pretrained(model_id)
            self.vlm = SmolVLMForConditionalGeneration(config=config)
        self.processor = AutoProcessor.from_pretrained(model_id)
        if num_vlm_layers > 0:
            print(f"Reducing the number of VLM layers to {num_vlm_layers} ...")
            self.get_vlm_model().text_model.layers = self.get_vlm_model().text_model.layers[:num_vlm_layers]
        self.num_vlm_layers = len(self.get_vlm_model().text_model.layers)          # 16
        self.config = config
        # Smaller lm expert
        lm_expert_config = copy.deepcopy(config.text_config)
        hidden_size = lm_expert_config.hidden_size                                 # ç»´åº¦æ˜¯ 960
        lm_expert_config.hidden_size = int(hidden_size * expert_width_multiplier)  # 480
        lm_expert_config.intermediate_size = get_intermediate_size(int(hidden_size * expert_width_multiplier)) # 1280
        lm_expert_config.num_hidden_layers = self.num_vlm_layers # 16
        if num_expert_layers > 0:
            assert len(self.get_vlm_model().text_model.layers) % num_expert_layers == 0, (
                f"Number of layers in the VLM {len(self.get_vlm_model().text_model.layers)} are not multiple of num_expert_layers {num_expert_layers}"
            )
            lm_expert_config.num_hidden_layers = num_expert_layers
        self.lm_expert = AutoModel.from_config(lm_expert_config)
        """
        lm_expert : 
        LlamaConfig {
            "_flash_attn_2_enabled": true,  # å¯ç”¨FlashAttention-2ä¼˜åŒ–, åŠ é€Ÿæ³¨æ„åŠ›è®¡ç®—å¹¶å‡å°‘å†…å­˜å ç”¨
            "architectures": [
                "VLlama3ForCausalLM"        # æ¨¡å‹æ¶æ„ä¸ºè§†è§‰-è¯­è¨€LLaMA 3çš„å› æœè¯­è¨€æ¨¡å‹å˜ä½“
            ],
            "attention_bias": false,        # æ³¨æ„åŠ›æœºåˆ¶ä¸­ä¸ä½¿ç”¨åç½®é¡¹
            "attention_dropout": 0.0,       # æ³¨æ„åŠ›å±‚ä¸åº”ç”¨dropout
            "bos_token_id": 1,              # å¼€å§‹æ ‡è®°(BOS)çš„IDä¸º1
            "eos_token_id": 2,              # ç»“æŸæ ‡è®°(EOS)çš„IDä¸º2
            "head_dim": 64,                 # æ¯ä¸ªæ³¨æ„åŠ›å¤´çš„ç»´åº¦å¤§å°
            "hidden_act": "silu",           # éšè—å±‚æ¿€æ´»å‡½æ•°ä½¿ç”¨SiLU (Swish)
            "hidden_size": 720,             # éšè—å±‚ç»´åº¦å¤§å°
            "initializer_range": 0.02,      # æƒé‡åˆå§‹åŒ–çš„èŒƒå›´
            "intermediate_size": 2048,      # MLPå±‚ä¸­é—´ç»´åº¦å¤§å°
            "is_llama_config": true,        # æ ‡è¯†ä¸ºLLaMAç³»åˆ—æ¨¡å‹é…ç½®
            "max_position_embeddings": 8192, # æœ€å¤§ä½ç½®ç¼–ç , æ”¯æŒ8192é•¿åº¦çš„åºåˆ—
            "mlp_bias": false,              # MLPå±‚ä¸ä½¿ç”¨åç½®é¡¹
            "model_type": "llama",          # æ¨¡å‹ç±»å‹ä¸ºLLaMA
            "neftune_noise_alpha": 0.0,     # ä¸å¯ç”¨NEFTuneå™ªå£°æ­£åˆ™åŒ–
            "num_attention_heads": 15,      # æ³¨æ„åŠ›å¤´çš„æ•°é‡
            "num_hidden_layers": 16,        # éšè—å±‚(Transformerå—)çš„æ•°é‡
            "num_key_value_heads": 5,       # ä½¿ç”¨Grouped Query Attention, æ¯ç»„5ä¸ªå¤´
            "pad_token_id": 2,              # å¡«å……æ ‡è®°(PAD)çš„IDä¸º2
            "perceiver_config": {           # è§†è§‰æ„ŸçŸ¥å™¨ç›¸å…³é…ç½®
                "_attn_implementation_autoset": false,
                "_name_or_path": "",
                "add_cross_attention": false,  # ä¸æ·»åŠ äº¤å‰æ³¨æ„åŠ›
                "architectures": null,
                "attention_dropout": 0.0,
                "bad_words_ids": null,
                "begin_suppress_tokens": null,
                "bos_token_id": null,
                "chunk_size_feed_forward": 0,
                "cross_attention_hidden_size": null,
                "decoder_start_token_id": null,
                "diversity_penalty": 0.0,
                "do_sample": false,
                "early_stopping": false,
                "encoder_no_repeat_ngram_size": 0,
                "eos_token_id": null,
                "exponential_decay_length_penalty": null,
                "finetuning_task": null,
                "forced_bos_token_id": null,
                "forced_eos_token_id": null,
                "hidden_act": "silu",
                "id2label": {
                "0": "LABEL_0",
                "1": "LABEL_1"
                },
                "is_decoder": false,
                "is_encoder_decoder": false,
                "label2id": {
                "LABEL_0": 0,
                "LABEL_1": 1
                },
                "length_penalty": 1.0,
                "max_length": 20,
                "min_length": 0,
                "model_type": "vllama3",
                "no_repeat_ngram_size": 0,
                "num_beam_groups": 1,
                "num_beams": 1,
                "num_key_value_heads": 1,     # æ„ŸçŸ¥å™¨é…ç½®ä¸­çš„KVå¤´æ•°é‡
                "num_return_sequences": 1,
                "output_attentions": false,
                "output_hidden_states": false,
                "output_scores": false,
                "pad_token_id": null,
                "prefix": null,
                "problem_type": null,
                "pruned_heads": {},
                "qk_layer_norms_perceiver": false,
                "remove_invalid_values": false,
                "repetition_penalty": 1.0,
                "resampler_depth": 6,         # è§†è§‰é‡é‡‡æ ·å™¨çš„å±‚æ•°
                "resampler_head_dim": 96,     # è§†è§‰é‡é‡‡æ ·å™¨çš„å¤´ç»´åº¦
                "resampler_n_heads": 16,      # è§†è§‰é‡é‡‡æ ·å™¨çš„å¤´æ•°é‡
                "resampler_n_latents": 64,    # è§†è§‰é‡é‡‡æ ·å™¨çš„æ½œåœ¨å˜é‡æ•°é‡
                "return_dict": true,
                "return_dict_in_generate": false,
                "sep_token_id": null,
                "suppress_tokens": null,
                "task_specific_params": null,
                "temperature": 1.0,
                "tf_legacy_loss": false,
                "tie_encoder_decoder": false,
                "tie_word_embeddings": true,
                "tokenizer_class": null,
                "top_k": 50,
                "top_p": 1.0,
                "torch_dtype": null,
                "torchscript": false,
                "transformers_version": "4.46.0",
                "typical_p": 1.0,
                "use_bfloat16": false
            },
            "pixel_shuffle_factor": 4,      # åƒç´ æ´—ç‰Œå› å­ï¼Œç”¨äºå¤„ç†è§†è§‰è¾“å…¥
            "pretraining_tp": 1,            # é¢„è®­ç»ƒæ—¶ä¸ä½¿ç”¨å¼ é‡å¹¶è¡Œ
            "qk_layer_norms": false,        # ä¸ä½¿ç”¨æŸ¥è¯¢å’Œé”®çš„å±‚å½’ä¸€åŒ–
            "rms_norm_eps": 1e-05,          # RMSNormçš„epsilonå€¼
            "rope_interleaved": false,      # RoPEä½ç½®ç¼–ç ä¸ä½¿ç”¨äº¤é”™æ’åˆ—
            "rope_scaling": null,           # ä¸ä½¿ç”¨RoPEç¼©æ”¾
            "rope_theta": 100000,           # RoPEçš„thetaå€¼, æ§åˆ¶ä½ç½®ç¼–ç çš„å‘¨æœŸ
            "tie_word_embeddings": false,   # ä¸å…±äº«è¾“å…¥å’Œè¾“å‡ºè¯åµŒå…¥
            "torch_dtype": "bfloat16",      # ä½¿ç”¨bfloat16æ•°æ®ç±»å‹è¿›è¡Œè®¡ç®—
            "transformers.js_config": {     # Transformers.jsçš„é…ç½®
                "kv_cache_dtype": {
                "fp16": "float16",
                "q4f16": "float16"
                }
            },
            "transformers_version": "4.50.3", # æ¨¡å‹ä½¿ç”¨çš„Transformersåº“ç‰ˆæœ¬
            "use_cache": true,                # å¯ç”¨KVç¼“å­˜ä»¥åŠ é€Ÿç”Ÿæˆ
            "use_resampler": false,           # ä¸ä½¿ç”¨è§†è§‰é‡é‡‡æ ·å™¨
            "vocab_size": 49280               # è¯è¡¨å¤§å°ä¸º49,280ä¸ªtoken
            }
        """

        self.num_expert_layers = len(self.lm_expert.layers)      # 16
        self.self_attn_every_n_layers = self_attn_every_n_layers # 2
        if "cross" in attention_mode:
            # Reshape qkv projections to have the same input dimension as the vlm
            # éå†è¯­è¨€æ¨¡å‹çš„æ¯ä¸€å±‚
            for layer_idx in range(len(self.lm_expert.layers)):
                # å¦‚æœå½“å‰å±‚ç´¢å¼•æ˜¯self_attn_every_n_layersçš„å€æ•°ï¼ˆä¾‹å¦‚ 0, 2, 4...ï¼‰ï¼Œåˆ™è·³è¿‡è¿™ä¸€å±‚ã€‚è¿™äº›å±‚å°†ä¿ç•™åŸå§‹çš„è‡ªæ³¨æ„åŠ›æœºåˆ¶
                if self.self_attn_every_n_layers > 0 and layer_idx % self.self_attn_every_n_layers == 0:
                    continue
                # å¯¹äºéœ€è¦ä¿®æ”¹çš„å±‚ï¼Œé‡æ–°å®šä¹‰å…¶é”®æŠ•å½±å±‚ï¼ˆk_projï¼‰ã€‚è¾“å…¥ç»´åº¦ä½¿ç”¨æ–‡æœ¬é…ç½®ä¸­çš„å¤´æ•°å’Œå¤´ç»´åº¦çš„ä¹˜ç§¯ï¼Œè¾“å‡ºç»´åº¦ä½¿ç”¨è¯­è¨€æ¨¡å‹ä¸“å®¶é…ç½®ä¸­çš„å¯¹åº”å€¼ã€‚è¿™æ ·åšæ˜¯ä¸ºäº†ä½¿è¯­è¨€æ¨¡å‹çš„é”®æŠ•å½±ç»´åº¦ä¸è§†è§‰è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰å…¼å®¹
                self.lm_expert.layers[layer_idx].self_attn.k_proj = nn.Linear(
                    config.text_config.num_key_value_heads * config.text_config.head_dim,
                    lm_expert_config.num_key_value_heads * lm_expert_config.head_dim,
                    bias=lm_expert_config.attention_bias,
                )
                # åŒæ ·åœ°ï¼Œé‡æ–°å®šä¹‰å€¼æŠ•å½±å±‚ï¼ˆv_projï¼‰ï¼Œä½¿å…¶è¾“å…¥è¾“å‡ºç»´åº¦ä¸é”®æŠ•å½±å±‚ä¸€è‡´ï¼Œç¡®ä¿æ¨¡å‹èƒ½å¤Ÿæ­£ç¡®å¤„ç†äº¤å‰æ³¨æ„åŠ›è®¡ç®—
                self.lm_expert.layers[layer_idx].self_attn.v_proj = nn.Linear(
                    config.text_config.num_key_value_heads * config.text_config.head_dim,
                    lm_expert_config.num_key_value_heads * lm_expert_config.head_dim,
                    bias=lm_expert_config.attention_bias,
                )
        # Remove unused embed_tokens
        self.lm_expert.embed_tokens = None

        self.num_attention_heads = self.config.text_config.num_attention_heads # 15
        self.num_key_value_heads = self.config.text_config.num_key_value_heads # 5


        # laq å‚æ•°
        self.chunk_size = chunk_size
        self.laq_code_seq_length = laq_code_seq_length
        self.laq_latent_dim = laq_latent_dim
        self.latent_head = nn.Linear(
            self.config.text_config.hidden_size,
            self.laq_latent_dim  
        )

        self.freeze_vision_encoder = freeze_vision_encoder
        self.train_expert_only = train_expert_only
        self.train_smolvlm_only = train_smolvlm_only
        self.lora_smolvlm = lora_smolvlm
        if self.lora_smolvlm:
            lora_cfg = LoraConfig(
                r = lora_r,
                lora_alpha = lora_alpha,
                target_modules=["q_proj", "k_proj", "v_proj","o_proj"],
                lora_dropout=lora_dropout,
                bias="none",
                task_type="SEQ_2_SEQ_LM"
            )
            # æˆ‘åªå¯¹smolvlm lora ï¼Œ expertå§‹ç»ˆå…¨é‡
            self.vlm = get_peft_model(self.vlm, lora_cfg)

        self.attention_mode = attention_mode
        self.expert_hidden_size = lm_expert_config.hidden_size
        self.set_requires_grad()

    def get_vlm_model(self):
        return self.vlm.model

    def set_requires_grad(self):
        # â€”â€” æ¨¡å¼ Aï¼šåªè®­ç»ƒ SmolVLMï¼ˆå†»ç»“ Expertï¼‰ â€”â€” #
        if self.train_smolvlm_only:
            # å…ˆå†»ç»“ VLM çš„æ‰€æœ‰å‚æ•°
            for p in self.vlm.parameters():
                p.requires_grad = False
            # å†æ ¹æ®æ˜¯å¦ç”¨ LoRAï¼Œè§£å†»å¯¹åº”å‚æ•°
            if self.lora_smolvlm:
               
                for n, p in self.vlm.named_parameters():
                    if "lora_" in n:
                        p.requires_grad = True
            else:
                # å…¨é‡å¾®è°ƒ VLM
                for p in self.vlm.parameters():
                    p.requires_grad = True
            # å†»ç»“ Expertï¼ˆåŠ¨ä½œå¤´ï¼‰
            for p in self.lm_expert.parameters():
                p.requires_grad = False
            # å¯é€‰ï¼šå†»ç»“è§†è§‰ encoder
            if self.freeze_vision_encoder:
                vision = self.get_vlm_model().vision_model
                vision.eval()
                for p in vision.parameters():
                    p.requires_grad = False
            return

        # å¦‚æœå†»ç»“ vlm vision å±‚ 
        if self.freeze_vision_encoder:
            self.get_vlm_model().vision_model.eval()
            for params in self.get_vlm_model().vision_model.parameters():
                params.requires_grad = False
        # å¦‚æœåªè®­ç»ƒåŠ¨ä½œå¤´
        if self.train_expert_only:
            self.vlm.eval()
            for params in self.vlm.parameters():
                params.requires_grad = False
        else:
            # To avoid unused params issue with distributed training
            last_layers = [self.num_vlm_layers - 1]
            if (
                self.num_vlm_layers != self.num_expert_layers
                and self.num_vlm_layers % self.num_expert_layers == 0
            ):
                last_layers.append(self.num_vlm_layers - 2)
            frozen_layers = [
                "lm_head",
                "text_model.model.norm.weight",
            ]
            for layer in last_layers:
                frozen_layers.append(f"text_model.model.layers.{layer}.")

            for name, params in self.vlm.named_parameters():
                if any(k in name for k in frozen_layers):
                    params.requires_grad = False

       
        # To avoid unused params issue with distributed training
        # "lm_head" é€šå¸¸æŒ‡çš„æ˜¯è¯­è¨€æ¨¡å‹ï¼ˆLanguage Modelï¼‰è¾“å‡ºå±‚ï¼Œå°†éšè—çŠ¶æ€æ˜ å°„ä¸ºè¯æ±‡è¡¨ä¸Šçš„æ¦‚ç‡åˆ†å¸ƒçš„çº¿æ€§å±‚
        for name, params in self.lm_expert.named_parameters():
            if "lm_head" in name:
                params.requires_grad = False

    def train(self, mode: bool = True):
        super().train(mode)

        if self.freeze_vision_encoder:
            self.get_vlm_model().vision_model.eval()

        if self.train_expert_only:
            self.vlm.eval()

    def embed_image(self, image: torch.Tensor):
        patch_attention_mask = None
        # Get sequence from the vision encoder
        image_hidden_states = (
            self.get_vlm_model()
            .vision_model(
                pixel_values=image.to(dtype=self.get_vlm_model().vision_model.dtype),
                patch_attention_mask=patch_attention_mask,
            )
            .last_hidden_state
        )
        # Modality projection & resampling
        image_hidden_states = self.get_vlm_model().connector(image_hidden_states)
        return image_hidden_states

    def embed_language_tokens(self, tokens: torch.Tensor):
        return self.get_vlm_model().text_model.get_input_embeddings()(tokens)
    

    def forward_latents(
        self,
        pixel_values: torch.FloatTensor,   # [B, C, 2, H, W]
        input_ids:     torch.LongTensor,    # [B, N]
        attention_mask:torch.LongTensor,    # [B, N]
    ) -> torch.FloatTensor:
        """
        ç»™å®šä¸ LAQ æ•™å¸ˆç›¸åŒçš„ pixel_values å’Œ decoder è¾“å…¥ (å…¨ BOS/å…¨ PAD)ï¼Œ
        è¾“å‡º [B, N, C, D] çš„é¢„æµ‹ latentsï¼Œç”¨äº MSE/CE ç›‘ç£ã€‚
        """   
        out = self.get_vlm_model().text_model(
            pixel_values=pixel_values,
            input_ids=input_ids,
            attention_mask=attention_mask,
            output_hidden_states=True,
            return_dict=True,
        )
        last_hidden = out.hidden_states[-1]              # [B, N, H]
        B,N,H = last_hidden.shape
        assert N == self.chunk_size * self.laq_code_seq_length
        flat = self.latent_head(last_hidden) # [B, N=chunksize * L, D]
        latents = flat.view(B, self.chunk_size, self.laq_code_seq_length, self.laq_latent_dim) # [B, chunksize, L, D]

        return latents
    """
    ğŸ’¡ æ³¨æ„åŠ›æµç¨‹å›¾

              Q (query_states)          K (key_states)         V (value_states)
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚ (B, L, H, D) â”‚         â”‚ (B, L, H, D) â”‚       â”‚ (B, L, H, D) â”‚
          â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚                          â”‚                      â”‚
         transpose(1,2)             transpose(1,2)               â”‚
                â†“                          â†“                      â†“
         (B, H, L, D)                (B, H, L, D)             (B, L, H, D)
                â”‚                          â”‚                      â”‚
                â””â”€â”€ Q Ã— Káµ€ = att_weights   â”‚                      â”‚
                           â†“              â”‚                      â”‚
                    (B, H, L, L)          â”‚                      â”‚
                           â†“              â”‚                      â†“
                 softmax(masked)          â”‚             V.permute(0,2,1,3) â†’ (B, H, L, D)
                           â†“              â”‚                      â†“
                probs = attention scores  â””â”€â”€â”€â”€â”€â”€â”€ matmul â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“
                      (B, H, L, L)
                           â†“
               att_output = probs @ V  â†’  (B, H, L, D)
                           â†“
             reshape â†’ (B, L, H*D) = final att_output âœ…

    where:
        B: batch_size
        L: token æ•°é‡ (å³ total_seq_len)
        H: æ³¨æ„åŠ›å¤´æ•° = num_attention_heads
        D: æ¯ä¸ªå¤´çš„ç»´åº¦ = head_dim
        hidden_dim = H Ã— D
    """
    def forward_attn_layer(
        self,
        model_layers,                         # æ¨¡å‹çš„å±‚é›†åˆï¼ˆå¯èƒ½åŒ…å«å¤šä¸ªæ¨¡å‹çš„å±‚ï¼Œè§†è§‰æ–‡æœ¬/åŠ¨ä½œä¸“å®¶ï¼‰
        inputs_embeds,                        # è¾“å…¥åµŒå…¥åˆ—è¡¨ï¼ˆæ¯ä¸ªå…ƒç´ æ˜¯ä¸€ç»„æ¨¡æ€çš„åµŒå…¥å‘é‡ï¼Œprefix/suffixï¼‰
        layer_idx,                            # å½“å‰å¤„ç†çš„å±‚ç´¢å¼•ï¼ˆåœ¨16å±‚ä¸­å®šä½åˆ°ç¬¬nå±‚ï¼‰
        position_ids,                         # ä½ç½®ç¼–ç IDï¼ˆç”¨äºRoPEä½ç½®ç¼–ç ï¼‰
        attention_mask,                       # æ³¨æ„åŠ›æ©ç ï¼ˆæ§åˆ¶å“ªäº›ä½ç½®å¯è¢«å…³æ³¨ï¼‰
        batch_size,                           # æ‰¹æ¬¡å¤§å°
        head_dim,                             # æ¯ä¸ªæ³¨æ„åŠ›å¤´çš„ç»´åº¦ï¼ˆå¦‚64ï¼‰
        use_cache: bool = True,               # æ˜¯å¦å¯ç”¨KVç¼“å­˜ï¼ˆç”Ÿæˆå¼ä»»åŠ¡åŠ é€Ÿï¼‰
        fill_kv_cache: bool = True,           # æ˜¯å¦å¡«å……KVç¼“å­˜ï¼ˆé¦–æ¬¡è®¡ç®—ä¸ºTrueï¼Œåç»­æ‹¼æ¥ä¸ºFalseï¼‰
        past_key_values=None,                 # å†å²KVç¼“å­˜ï¼ˆå­˜å‚¨ä¹‹å‰æ­¥éª¤çš„key/valueçŠ¶æ€ï¼‰
    ) -> list[torch.Tensor]:                  # è¿”å›æ³¨æ„åŠ›è¾“å‡ºå’Œæ›´æ–°åçš„KVç¼“å­˜

        query_states = []                     # å­˜å‚¨æ‰€æœ‰è¾“å…¥æ¨¡æ€çš„æŸ¥è¯¢ï¼ˆQueryï¼‰çŠ¶æ€
        key_states = []                       # å­˜å‚¨æ‰€æœ‰è¾“å…¥æ¨¡æ€çš„é”®ï¼ˆKeyï¼‰çŠ¶æ€
        value_states = []                     # å­˜å‚¨æ‰€æœ‰è¾“å…¥æ¨¡æ€çš„å€¼ï¼ˆValueï¼‰çŠ¶æ€

        """
        inputs_embeds æ˜¯ä¸€ä¸ª list
            inputs_embeds[0]: prefix     [B, L1, 960]
            inputs_embeds[1]: suffix     [B, L2, 720]
        """
        for i, hidden_states in enumerate(inputs_embeds):
            # è·å–å¯¹åº”æ¨¡æ€åœ¨å½“å‰å±‚ï¼ˆlayer_idxï¼‰çš„Transformerå±‚
            layer = model_layers[i][layer_idx]
            if hidden_states is None or layer is None:
                continue

            # å¯¹è¾“å…¥è¿›è¡Œå±‚å½’ä¸€åŒ–ï¼ˆè‡ªæ³¨æ„åŠ›å‰çš„é¢„å¤„ç†ï¼‰
            hidden_states = layer.input_layernorm(hidden_states)

            # è®¡ç®—éšè—çŠ¶æ€çš„å½¢çŠ¶ï¼š(batch_size, seq_len, ...) â†’ ä¿ç•™å‰ä¸¤ç»´ï¼ˆbatchå’Œåºåˆ—é•¿åº¦ï¼‰
            input_shape = hidden_states.shape[:-1]
            # é‡å¡‘å½¢çŠ¶ä¸º (batch_size, seq_len, num_heads, head_dim)ï¼Œç”¨äºåç»­æ³¨æ„åŠ›å¤´æ‹†åˆ† head_dim éƒ½æ˜¯ 64

            # hidden_shape : (B, L, -1, 64)
            hidden_shape = (*input_shape, -1, layer.self_attn.head_dim)

            # ç¡®ä¿è¾“å…¥ä¸æŠ•å½±å±‚æƒé‡çš„æ•°æ®ç±»å‹ä¸€è‡´ï¼ˆå¦‚bfloat16ï¼‰
            hidden_states = hidden_states.to(dtype=layer.self_attn.q_proj.weight.dtype)

            # é€šè¿‡çº¿æ€§æŠ•å½±å±‚è®¡ç®—Qã€Kã€Vï¼Œå¹¶é‡å¡‘ä¸ºæ³¨æ„åŠ›å¤´æ ¼å¼
            """
            vlm:
                Q: (B=8, L=197, H=15, D=64)  â†’ æ€»ç»´åº¦ 960
                K: (B=8, L=197, H=5,  D=64)  â†’ æ€»ç»´åº¦ 320
                V: (B=8, L=197, H=5,  D=64)  â†’ æ€»ç»´åº¦ 320
            lm_expert:
                query_state shape  : torch.Size([8, 50, 15, 64])
                key_state shape    : torch.Size([8, 50, 5, 64])
                value_state shape  : torch.Size([8, 50, 5, 64])
            """
            query_state = layer.self_attn.q_proj(hidden_states).view(hidden_shape)
            key_state = layer.self_attn.k_proj(hidden_states).view(hidden_shape)
            value_state = layer.self_attn.v_proj(hidden_states).view(hidden_shape)



            query_states.append(query_state)
            key_states.append(key_state)
            value_states.append(value_state)

        # B,L,H,D with L sequence length, H number of heads, D head dim
        # concatenate on the number of embeddings/tokens

        # æ‹¼æ¥æ‰€æœ‰æ¨¡æ€çš„ Q/K/V â†’ åœ¨åºåˆ—é•¿åº¦ç»´åº¦æ‹¼æ¥ï¼Œè€Œä¸æ˜¯ head æ•°ç»´åº¦
        query_states = torch.cat(query_states, dim=1)   # (B, L_total, H_q, D)
        key_states = torch.cat(key_states, dim=1)       # (B, L_total, H_k, D)
        value_states = torch.cat(value_states, dim=1)   # (B, L_total, H_k, D)
        # è·å–æ‹¼æ¥åçš„æ€»åºåˆ—é•¿åº¦
        seq_len = query_states.shape[1]
        # è°ƒæ•´ä½ç½®ç¼–ç å’Œæ³¨æ„åŠ›æ©ç çš„é•¿åº¦ï¼ˆç¡®ä¿ä¸æ‹¼æ¥åçš„åºåˆ—é•¿åº¦åŒ¹é…ï¼‰
        if seq_len < position_ids.shape[1]:
            # è‹¥æ‹¼æ¥ååºåˆ—æ›´çŸ­ï¼Œæˆªæ–­ä½ç½®ç¼–ç å’Œæ©ç 
            _position_ids = position_ids[:, :seq_len]
            _attention_mask = attention_mask[:, :seq_len, :seq_len]
        else:
            # å¦åˆ™ä½¿ç”¨åŸå§‹ä½ç½®ç¼–ç å’Œæ©ç 
            _position_ids = position_ids
            _attention_mask = attention_mask

        # é‡å‘½åå˜é‡
        attention_mask_ = _attention_mask
        position_ids_ = _position_ids
        # å¯¹Qå’ŒKåº”ç”¨æ—‹è½¬ä½ç½®ç¼–ç ï¼ˆRoPEï¼‰ï¼Œæ³¨å…¥ä½ç½®ä¿¡æ¯
        query_states = apply_rope(query_states, position_ids_)
        key_states = apply_rope(key_states, position_ids_)

        # åˆå§‹åŒ–KVç¼“å­˜ï¼ˆè‹¥å¯ç”¨ç¼“å­˜ä¸”ç¼“å­˜ä¸ºç©ºï¼‰
        if use_cache and past_key_values is None:
            past_key_values = {}

        if use_cache:
            if fill_kv_cache:
                # é¦–æ¬¡è®¡ç®—ï¼šå°†å½“å‰å±‚çš„Kã€Vå­˜å…¥ç¼“å­˜
                past_key_values[layer_idx] = {
                    "key_states": key_states,
                    "value_states": value_states,
                }
            else:
                # åç»­ç”Ÿæˆæ­¥éª¤ï¼šå°†æ–°çš„Kã€Vä¸å†å²ç¼“å­˜æ‹¼æ¥ï¼ˆå¦‚ autoregressive ç”Ÿæˆæ—¶ç´¯åŠ åºåˆ—ï¼‰
                # TODO here, some optimization can be done - similar to a `StaticCache` we can declare the `max_len` before.
                # so we create an empty cache, with just one cuda malloc, and if (in autoregressive case) we reach
                # the max len, then we (for instance) double the cache size. This implementation already exists
                # in `transformers`. (molbap)
                key_states = torch.cat([past_key_values[layer_idx]["key_states"], key_states], dim=1)
                value_states = torch.cat([past_key_values[layer_idx]["value_states"], value_states], dim=1)

        # è·å–æ³¨æ„åŠ›è®¡ç®—æ¥å£
        attention_interface = self.get_attention_interface()

        # è°ƒç”¨æ³¨æ„åŠ›æ¥å£è®¡ç®—æ³¨æ„åŠ›è¾“å‡º
        att_output = attention_interface(
            attention_mask_,  # æ³¨æ„åŠ›æ©ç 
            batch_size,       # æ‰¹æ¬¡å¤§å°
            head_dim,         # å¤´ç»´åº¦
            query_states,     # (B, Lq, Hq, D)
            key_states,       # (B, Lk, Hk, D)
            value_states      # (B, Lv, Hk, D)
        )
        # è¿”å›æ³¨æ„åŠ›è¾“å‡ºï¼ˆåˆ—è¡¨å½¢å¼ï¼Œä¾¿äºåç»­å¤„ç†ï¼‰å’Œæ›´æ–°åçš„KVç¼“å­˜
        return [att_output], past_key_values



    """
    Prefix æ¨¡æ€ (æ¨¡æ€1):
        Q_prefix (B, L1, H1, D)         K_prefix (B, L1, H1, D)        V_prefix (B, L1, H1, D)
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚              â”‚             â”‚              â”‚             â”‚              â”‚
            â”‚  input_layernorm             â”‚  input_layernorm             â”‚  input_layernorm
            â”‚  + q_proj                    â”‚  + k_proj                    â”‚  + v_proj
            â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚                            â”‚                            â”‚
                transpose(1,2)                transpose(1,2)                (no transpose)
                    â†“                            â†“                            â†“
                (B, H1, L1, D)                (B, H1, L1, D)                (B, L1, H1, D)

                Q_prefix Ã— K_prefixáµ€  â†’ att_weights_prefix
                    â†“
                (B, H1, L1, L1)
                    â†“
                softmax(masked)
                    â†“
                att_output_prefix = att_weights_prefix @ V_prefix
                    â†“
                (B, H1, L1, D)
                    â†“
                reshape â†’ (B, L1, H1*D)


    Expert æ¨¡æ€ (æ¨¡æ€2):
        Q_expert (B, L2, H2, D)         K_expert (B, L1, H2, D)         V_expert (B, L1, H2, D)
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚  input_layernorm             â”‚ key_states from prefix     â”‚ value_states from prefix
            â”‚  + q_proj (expert)           â”‚ + expert k_proj           â”‚ + expert v_proj
            â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚                            â”‚                            â”‚
                transpose(1,2)                transpose(1,2)                (no transpose)
                    â†“                            â†“                            â†“
                (B, H2, L2, D)                (B, H2, L1, D)                (B, L1, H2, D)

                Q_expert Ã— K_expertáµ€  â†’ att_weights_expert
                    â†“
                (B, H2, L2, L1)
                    â†“
                softmax(masked)
                    â†“
                att_output_expert = att_weights_expert @ V_expert
                    â†“
                (B, H2, L2, D)
                    â†“
                reshape â†’ (B, L2, H2*D)

    """

    
    def forward_cross_attn_layer(
        self,
        model_layers,
        inputs_embeds,
        layer_idx,
        position_ids,
        attention_mask,
        batch_size,
        head_dim,
        use_cache: bool = True,
        fill_kv_cache: bool = True,
        past_key_values=None,
    ) -> list[torch.Tensor]:
        attention_interface = self.get_attention_interface()

        att_outputs = []
        assert len(inputs_embeds) == 2 or (use_cache and past_key_values is not None and not fill_kv_cache), (
            f"Both len(inputs_embeds) == {len(inputs_embeds)} and past_key_values is {past_key_values}"
        )

        if len(inputs_embeds) == 2 and not past_key_values:
            # åœºæ™¯1ï¼šæœ‰ä¸¤ä¸ªè¾“å…¥æ¨¡æ€ä¸”æ— å†å²ç¼“å­˜ï¼ˆé¦–æ¬¡è®¡ç®—ï¼‰
            # Prefix attention

            # åˆ†å‰²ä½ç½®ç¼–ç å’Œæ³¨æ„åŠ›æ©ç ï¼ˆinputs_embeds[0]æ˜¯å‰ç¼€ï¼Œinputs_embeds[1]æ˜¯ä¸“å®¶ï¼‰
            seq_len = inputs_embeds[0].shape[1]
            position_id, expert_position_id = position_ids[:, :seq_len], position_ids[:, seq_len:]
            prefix_attention_mask = attention_mask[:, :seq_len, :seq_len]

            # è·å–ç¬¬ä¸€å±‚ï¼ˆå‰ç¼€å±‚ï¼‰å¹¶å¤„ç†
            layer = model_layers[0][layer_idx]

            # å±‚å½’ä¸€åŒ–å’ŒQKVæŠ•å½±
            hidden_states = layer.input_layernorm(inputs_embeds[0])

            input_shape = hidden_states.shape[:-1]
            hidden_shape = (*input_shape, -1, layer.self_attn.head_dim)

            hidden_states = hidden_states.to(dtype=layer.self_attn.q_proj.weight.dtype)

            # è®¡ç®—QKVå¹¶åº”ç”¨RoPEä½ç½®ç¼–ç 
            query_state = layer.self_attn.q_proj(hidden_states).view(hidden_shape)
            key_state = layer.self_attn.k_proj(hidden_states).view(hidden_shape)
            value_states = layer.self_attn.v_proj(hidden_states).view(hidden_shape)

            # B,L,H,D with L sequence length, H number of heads, D head dim
            query_states = apply_rope(query_state, position_id)
            key_states = apply_rope(key_state, position_id)

            # è®¡ç®—è‡ªæ³¨æ„åŠ›ï¼ˆå‰ç¼€å†…éƒ¨çš„æ³¨æ„åŠ›ï¼‰
            att_output = attention_interface(
                prefix_attention_mask, batch_size, head_dim, query_states, key_states, value_states
            )
            att_outputs.append(att_output)
        else:
            # åœºæ™¯2ï¼šä½¿ç”¨ç¼“å­˜æˆ–ä»…æœ‰ä¸€ä¸ªè¾“å…¥æ¨¡æ€
            expert_position_id = position_ids

        if use_cache and past_key_values is None:
            past_key_values = {}

        if use_cache:
            if fill_kv_cache:
                past_key_values[layer_idx] = {
                    "key_states": key_states,
                    "value_states": value_states,
                }
            else:
                # TODO here, some optimization can be done - similar to a `StaticCache` we can declare the `max_len` before.
                # so we create an empty cache, with just one cuda malloc, and if (in autoregressive case) we reach
                # the max len, then we (for instance) double the cache size. This implementation already exists
                # in `transformers`. (molbap)
                key_states = past_key_values[layer_idx]["key_states"]
                value_states = past_key_values[layer_idx]["value_states"]

        # Expert
        expert_layer = model_layers[1][layer_idx]
        if expert_layer is not None:
            expert_hidden_states = expert_layer.input_layernorm(inputs_embeds[1])

            expert_input_shape = expert_hidden_states.shape[:-1]
            expert_hidden_shape = (*expert_input_shape, -1, expert_layer.self_attn.head_dim)

            expert_hidden_states = expert_hidden_states.to(dtype=expert_layer.self_attn.q_proj.weight.dtype)
            expert_query_state = expert_layer.self_attn.q_proj(expert_hidden_states).view(expert_hidden_shape)

            _key_states = key_states.to(dtype=expert_layer.self_attn.k_proj.weight.dtype).view(
                *key_states.shape[:2], -1
            )
            expert_key_states = expert_layer.self_attn.k_proj(_key_states).view(
                *_key_states.shape[:-1], -1, expert_layer.self_attn.head_dim
            )  # k_proj should have same dim as kv

            _value_states = value_states.to(dtype=expert_layer.self_attn.v_proj.weight.dtype).view(
                *value_states.shape[:2], -1
            )
            expert_value_states = expert_layer.self_attn.v_proj(_value_states).view(
                *_value_states.shape[:-1], -1, expert_layer.self_attn.head_dim
            )

            expert_position_id = (
                expert_position_id - torch.min(expert_position_id, dim=1, keepdim=True).values
            )  # start from 0
            expert_attention_mask = attention_mask[
                :, -inputs_embeds[1].shape[1] :, : expert_key_states.shape[1] :
            ]  # take into account kv

            expert_query_states = apply_rope(expert_query_state, expert_position_id)

            att_output = attention_interface(
                expert_attention_mask,
                batch_size,
                head_dim,
                expert_query_states,
                expert_key_states,
                expert_value_states,
            )
            att_outputs.append(att_output)
        else:
            att_outputs.append(None)

        # att_output = att_output.to(dtype=models[i].dtype)
        return att_outputs, past_key_values

    def get_model_layers(self, models: list) -> list:
        vlm_layers = []
        expert_layers = []
        multiple_of = self.num_vlm_layers // self.num_expert_layers
        for i in range(self.num_vlm_layers):
            if multiple_of > 0 and i > 0 and i % multiple_of != 0:
                expert_layer = None
            else:
                expert_layer_index = i // multiple_of if multiple_of > 0 else i
                expert_layer = models[1].layers[expert_layer_index]
            vlm_layers.append(models[0].layers[i])
            expert_layers.append(expert_layer)
        return [vlm_layers, expert_layers]




    """
    ğŸ¯ åˆå¹¶æ€»ç»“ï¼š
        | Layer | Self / Cross | Expert å­˜åœ¨ï¼Ÿ | æ‰§è¡Œå‡½æ•°                     | è¯´æ˜                                          
        | ----- | ------------ | ----------   | --------------------------- | ----------------------------------------        
        | 0     | Self         | âœ…           | forward\_attn\_layer        | vlmå’Œexpertéƒ½åšself-attn, å„è‡ªç‹¬ç«‹                 
        | 1     | Cross        | âœ…           | forward\_cross\_attn\_layer | expertç”¨vlmçš„kvè®¡ç®—cross-attn (queryæ¥è‡ªexpert)   
        | 2     | Self         | âœ…           | forward\_attn\_layer        | åŒä¸Š                                             
        | 3     | Cross        | âœ…           | forward\_cross\_attn\_layer | åŒä¸Š                                             
        | ...   | ...          | ...          | ...                         | ...                                              
        | 14    | Self         | âœ…           | forward\_attn\_layer        | åŒä¸Š                                       
        | 15    | Cross        | âœ…           | forward\_cross\_attn\_layer | åŒä¸Š                                       

    """
    def forward(
        self,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_values: Optional[List[torch.FloatTensor]] = None,
        inputs_embeds: List[torch.FloatTensor] = None,
        use_cache: Optional[bool] = None,
        fill_kv_cache: Optional[bool] = None,
    ):
        """
        ğŸ” å«ä¹‰ï¼š
        è·å–å¤šä¸ªæ¨¡å‹çš„æ–‡æœ¬éƒ¨åˆ†ç»„æˆçš„åˆ—è¡¨ï¼šä¸€ä¸ªæ¥è‡ª VLM (Vision-Language Model), å¦ä¸€ä¸ªæ˜¯ LM Expert (åŠ¨ä½œä¸“å®¶)
        è°ƒç”¨ get_model_layers() æ–¹æ³•è·å–æ¯ä¸ª Transformer å±‚ï¼Œå½¢æˆåµŒå¥—ç»“æ„ï¼š
            model_layers = [
                [layer_0_model_0, layer_1_model_0, ..., layer_n_model_0],
                [layer_0_model_1, layer_1_model_1, ..., layer_n_model_1],
            ]
        """
        models = [self.get_vlm_model().text_model, self.lm_expert]
        model_layers = self.get_model_layers(models)
        # è·å– batchsize
        for hidden_states in inputs_embeds:
            # TODO this is very inefficient
            # dtype is always the same, batch size too (if > 1 len)
            # device could be trickier in multi gpu edge cases but that's it
            if hidden_states is None:
                continue
            batch_size = hidden_states.shape[0]

        # RMSNorm
        # vlm backbone çš„å±‚æ•°
        num_layers = self.num_vlm_layers                        # num_layers = 16
        head_dim = self.vlm.config.text_config.head_dim         # head_dim = 64
        """
        Layer 0:  self-attn
        Layer 1:  cross-attn
        Layer 2:  self-attn
        Layer 3:  cross-attn
        ...
        Layer 15: cross-attn
        """
        for layer_idx in range(num_layers):
            if (
                fill_kv_cache
                or "cross" not in self.attention_mode
                or (self.self_attn_every_n_layers > 0 and layer_idx % self.self_attn_every_n_layers == 0)
            ):
                print
                att_outputs, past_key_values = self.forward_attn_layer(
                    model_layers,
                    inputs_embeds,
                    layer_idx,
                    position_ids,
                    attention_mask,
                    batch_size,
                    head_dim,
                    use_cache=use_cache,
                    fill_kv_cache=fill_kv_cache,
                    past_key_values=past_key_values,
                )
            else:
                att_outputs, past_key_values = self.forward_cross_attn_layer(
                    model_layers,
                    inputs_embeds,
                    layer_idx,
                    position_ids,
                    attention_mask,
                    batch_size,
                    head_dim,
                    use_cache=use_cache,
                    fill_kv_cache=fill_kv_cache,
                    past_key_values=past_key_values,
                )
            outputs_embeds = []
            start = 0
            for i, hidden_states in enumerate(inputs_embeds):
                layer = model_layers[i][layer_idx]
                att_output = (
                    att_outputs[i] if i < len(att_outputs) else att_outputs[0]
                )  # in case of self_attn
                if hidden_states is not None:
                    if layer is None:
                        outputs_embeds.append(hidden_states)
                        continue
                    end = start + hidden_states.shape[1]

                    if att_output.dtype != layer.self_attn.o_proj.weight.dtype:
                        att_output = att_output.to(layer.self_attn.o_proj.weight.dtype)
                    att_out = att_output[:, start:end]
                    out_emb = layer.self_attn.o_proj(att_out)

                    out_emb += hidden_states
                    after_first_residual = out_emb.clone()

                    out_emb = layer.post_attention_layernorm(out_emb)
                    out_emb = layer.mlp(out_emb)

                    out_emb += after_first_residual

                    outputs_embeds.append(out_emb)

                    start = end if len(att_outputs) == 1 else 0
                else:
                    outputs_embeds.append(None)

            inputs_embeds = outputs_embeds

        # final norm
        outputs_embeds = []
        for i, hidden_states in enumerate(inputs_embeds):
            if hidden_states is not None:
                out_emb = models[i].norm(hidden_states)
                outputs_embeds.append(out_emb)
            else:
                outputs_embeds.append(None)
        return outputs_embeds, past_key_values

    def get_attention_interface(self):
        attention_interface = self.eager_attention_forward
        return attention_interface

    def eager_attention_forward(
        self, attention_mask, batch_size, head_dim, query_states, key_states, value_states
    ):
        num_att_heads = self.num_attention_heads
        num_key_value_heads = self.num_key_value_heads
        num_key_value_groups = num_att_heads // num_key_value_heads

        sequence_length = key_states.shape[1]

        key_states = key_states[:, :, :, None, :].expand(
            batch_size, sequence_length, num_key_value_heads, num_key_value_groups, head_dim
        )
        key_states = key_states.reshape(
            batch_size, sequence_length, num_key_value_heads * num_key_value_groups, head_dim
        )

        value_states = value_states[:, :, :, None, :].expand(
            batch_size, sequence_length, num_key_value_heads, num_key_value_groups, head_dim
        )
        value_states = value_states.reshape(
            batch_size, sequence_length, num_key_value_heads * num_key_value_groups, head_dim
        )

        # Attention here is upcasted to float32 to match the original eager implementation.
        query_states = query_states.to(dtype=torch.float32)
        key_states = key_states.to(dtype=torch.float32)

        query_states = query_states.transpose(1, 2)
        key_states = key_states.transpose(1, 2)

        att_weights = torch.matmul(query_states, key_states.transpose(2, 3))
        att_weights *= head_dim**-0.5

        att_weights = att_weights.to(dtype=torch.float32)
        big_neg = torch.finfo(att_weights.dtype).min  # -2.3819763e38  # See gemma/modules.py
        masked_att_weights = torch.where(attention_mask[:, None, :, :], att_weights, big_neg)
        probs = nn.functional.softmax(masked_att_weights, dim=-1)
        probs = probs.to(dtype=value_states.dtype)

        att_output = torch.matmul(probs, value_states.permute(0, 2, 1, 3))

        att_output = att_output.permute(0, 2, 1, 3)
        # we use -1 because sequence length can change
        att_output = att_output.reshape(batch_size, -1, num_key_value_heads * num_key_value_groups * head_dim)

        return att_output
