from pathlib import Path
import math

import torch
import torch.nn.functional as F
from torch import nn
from einops import rearrange, pack
from einops.layers.torch import Rearrange
from einops import repeat

from lerobot.common.policies.egovla.laq.laq_model.attention import Transformer, ContinuousPositionBias
from lerobot.common.policies.egovla.laq.laq_model.nsvq import NSVQ
# from lerobot.common.policies.egovla.laq.laq_model.piper_arm import PiperArm
# from scipy.spatial.transform import Rotation as R

def exists(val):
    return val is not None


def pair(val):
    ret = (val, val) if not isinstance(val, tuple) else val
    assert len(ret) == 2
    return ret


class LatentActionQuantization(nn.Module):
    def __init__(
        self,
        *,
        dim,                        # Transformer ä¸­çš„ç‰¹å¾ç»´åº¦
        quant_dim,                  # é‡åŒ–åµŒå…¥å‘é‡ç»´åº¦
        codebook_size,              # å‘é‡é‡åŒ–çš„ç æœ¬å¤§å°
        image_size,                 # è¾“å…¥å›¾åƒå¤§å°
        patch_size,                 # åˆ†å‰²æˆ patch çš„å¤§å°
        spatial_depth,              # ç©ºé—´æ–¹å‘ Transformer å±‚æ•°
        temporal_depth,             # æ—¶é—´æ–¹å‘ Transformer å±‚æ•°
        dim_head = 64,
        heads = 8,
        channels = 3,
        attn_dropout = 0.,
        ff_dropout = 0.,
        code_seq_len = 1,           #  é‡åŒ–ä»£ç é•¿åº¦
        is_use_action_state = False,
        is_use_fk_state = False,    # æ˜¯å¦ä½¿ç”¨ FK çŠ¶æ€
        state_dim = 16,         # action state çš„ç»´åº¦
        chunksize = 50,
        ):
        """
        einstein notations:

        b - batch
        c - channels
        t - time
        d - feature dimension
        p1, p2, pt - image patch sizes and then temporalNSVQ patch size
        """

        super().__init__()

        self.code_seq_len = code_seq_len
        self.chunksize = chunksize
        self.state_dim    = state_dim
        """
        pair:
            å¦‚æœ x å·²ç»æ˜¯ä¸€ä¸ªå½¢å¦‚ (height, width) çš„ tuple, å°±ç›´æ¥è¿”å›
            å¦‚æœ x æ˜¯ä¸€ä¸ª å•ç‹¬çš„æ•´æ•° (æ¯”å¦‚ 256), å°±å°†å…¶è½¬æ¢æˆ (256, 256) çš„å½¢å¼
        """
        self.image_size = pair(image_size)
        self.patch_size = pair(patch_size)

        patch_height, patch_width = self.patch_size

        # è¿ç»­ç©ºé—´ç›¸å¯¹ä½ç½®åç½®å­æ¨¡å—
        self.spatial_rel_pos_bias = ContinuousPositionBias(dim = dim, heads = heads)

        # ä¿è¯å›¾åƒçš„å®½é«˜ä¸ä¸º 0
        image_height, image_width = self.image_size
        assert (image_height % patch_height) == 0 and (image_width % patch_width) == 0

        """
        to_patch_emb_first_frame
            è¾“å…¥:
            [b, c, 1, H, W]

            Step 1: Rearrange åˆ‡åˆ† patch
            [b, c, 1, H, W]
                => Rearrange (b c 1 (h p1) (w p2) -> b 1 h w (c p1 p2))

                h = H//p1 w = W//p2
            [b, 1, h, w, (c * p1 * p2)]

            Step 2: LayerNorm å¯¹æœ€åä¸€ç»´ [..., 3072] åš æ ‡å‡†åŒ–
            [b, 1, h, w, (c * p1 * p2)]

            Step 3: Linearæ˜ å°„
            [b, 1, h, w, dim]

            Step 4: LayerNorm
            [b, 1, h, w, dim]

            è¾“å‡º:
            [b, 1, h, w, dim]
        """
        self.to_patch_emb_first_frame = nn.Sequential(
            Rearrange('b c 1 (h p1) (w p2) -> b 1 h w (c p1 p2)', p1 = patch_height, p2 = patch_width),
            nn.LayerNorm(channels * patch_width * patch_height),
            nn.Linear(channels * patch_width * patch_height, dim),
            nn.LayerNorm(dim)
        )
        

        transformer_kwargs = dict(
            dim = dim,
            dim_head = dim_head,
            heads = heads,
            attn_dropout = attn_dropout,
            ff_dropout = ff_dropout,
            peg = True,
            peg_causal = True,
        )
        
        transformer_with_action_kwargs = dict(
            dim = dim,
            dim_head = dim_head,
            heads = heads,
            attn_dropout = attn_dropout,
            ff_dropout = ff_dropout,
            peg = True,
            peg_causal = True,
            has_cross_attn = True,
            dim_context = dim,
        )

        """ ç©ºé—´ Transformer """
        self.enc_spatial_transformer = Transformer(depth = spatial_depth, **transformer_kwargs)
        """ æ—¶é—´ Transformer """
        self.enc_temporal_transformer = Transformer(depth = temporal_depth, **transformer_kwargs)

        """ å‘é‡é‡åŒ–æ¨¡å— """
        self.vq = NSVQ(
            dim=dim,
            num_embeddings=codebook_size,      # 8
            embedding_dim=quant_dim,           # 32
            device='cuda',
            code_seq_len=code_seq_len,         # 4
            patch_size=patch_size,             # 32
            image_size=image_size              # 256
        )
            
        # è§£ç æ¨¡å—
        self.dec_spatial_transformer = Transformer(depth = spatial_depth, **transformer_with_action_kwargs)
        # [b, 1, h, w, dim] -> [b, c, 1, h * p1, w * p2]
        self.to_pixels_first_frame = nn.Sequential(
            nn.Linear(dim, channels * patch_width * patch_height),
            Rearrange('b 1 h w (c p1 p2) -> b c 1 (h p1) (w p2)', p1 = patch_height, p2 = patch_width)
        )

        # å¦‚æœä½¿ç”¨ action state
        self.is_use_action_state = is_use_action_state
        self.is_use_fk_state = is_use_fk_state
        # if is_use_action_state == True:
        #     self.action_state_proj = nn.Linear(16, 1024)
        if self.is_use_action_state:
            # å…ˆæŠŠç¬¬ä¸€å¸§çš„åŸå§‹ state æŠ•å½±åˆ°å’Œ patch-emb åŒç»´åº¦
            self.action_state_proj = nn.Linear(state_dim, dim)

            self.dim = dim

            # State â†’ token åºåˆ—çš„æŠ•å½±
            self.state_to_tokens = nn.Sequential(
                nn.LayerNorm(dim),
                nn.Linear(dim, dim * code_seq_len),
                nn.ReLU()
            )

            self.state_cross_attn = nn.MultiheadAttention(embed_dim=dim, num_heads=heads, batch_first=True)
           
            # â€”â€” **æ–°å¢ï¼šFiLM æ¡ä»¶ç”Ÿæˆå™¨** â€”â€”
            # è¾“å…¥ state_dimï¼Œè¾“å‡º 2Ã—dim çš„ gamma/beta
            self.film_generator = nn.Sequential(
                nn.Linear(dim, dim * 2),
                nn.ReLU(),
            )
            # â€”â€” **End FiLM** â€”â€”
            # ---- state decoder ----
            self.state_decoder = nn.Sequential(
                nn.LayerNorm(dim),
                nn.Linear(dim, dim),
                nn.ReLU(),
                nn.Linear(dim, chunksize * state_dim),
                nn.Sigmoid()  # å¼ºåˆ¶å½’ä¸€åŒ–åˆ° [0, 1]
            )

            self.state_fk_decoder = nn.Sequential(
                nn.LayerNorm(dim),
                nn.Linear(dim, dim),
                nn.ReLU(),
                nn.Linear(dim, chunksize * 14),
                nn.Sigmoid()  # å¼ºåˆ¶å½’ä¸€åŒ–åˆ° [0, 1]
            )

            self.is_use_delta_action_state = True


    def state_dict(self, *args, **kwargs):
        return super().state_dict(*args, **kwargs)

    def load_state_dict(self, *args, **kwargs):
        return super().load_state_dict(*args, **kwargs, strict = False)

    def load(self, path):
        path = Path(path)
        assert path.exists()
        pt = torch.load(str(path))
        pt = {k.replace('module.', '') if 'module.' in k else k: v for k, v in pt.items()}
        self.load_state_dict(pt)

    def decode_from_codebook_indices(self, indices):
        codes = self.vq.codebooks[indices]

        return codes

    @property
    def patch_height_width(self):
        return self.image_size[0] // self.patch_size[0], self.image_size[1] // self.patch_size[1]

    def encode(
        self,
        tokens,
        state_token=None
    ):
        # b   : batch size
        b = tokens.shape[0]
        d = tokens.shape[-1]
        # h, w: patch grid çš„é«˜åº¦å’Œå®½åº¦ï¼ˆæ¯”å¦‚ 8Ã—8ï¼‰
        h, w = self.patch_height_width
        # video_shape: è®°å½• [b, t, h, w] æ–¹ä¾¿åç»­ reshape ç”¨
        video_shape = tuple(tokens.shape[:-1])
        first_state_token = state_token[:,:self.dim]
        last_state_token = state_token[:,self.dim:]

        state_token = None


        """
        ğŸ“Œ ç›®çš„ï¼šæŠŠæ¯ä¸€å¸§å½“ä½œä¸€ä¸ªå›¾åƒå¤„ç†ï¼Œé€å…¥ç©ºé—´ Transformer
                æŠŠæ¯ä¸€å¸§ flatten æˆ patch sequence:
                    è¾“å…¥ï¼š[b, t, h, w, d] â†’ è¾“å‡ºï¼š[(bÃ—t), (hÃ—w), d]
                ä¸¾ä¾‹:
                    å¦‚æœ b=1, t=2, h=8, w=8, d=1024
                    å°±ä¼šå˜æˆ [2, 64, 1024]
        """
        # Film èåˆtokenså’Œç¬¬ä¸€å¸§çš„state ï¼š https://arxiv.org/abs/1709.07871 FiLM: Visual Reasoning with a General Conditioning Layer
        if state_token is not None:
            # state_token: [B, D] â†’  [B, 1, D]
            first_ctx = first_state_token.unsqueeze(1)
            first_vis_flat = rearrange(tokens[:,:1], 'b t h w d -> b (t h w) d')  # [B, L, D], L=hÂ·wÂ·t
            first_fuse, _ = self.state_cross_attn(first_vis_flat, first_ctx, first_ctx)

            last_ctx = last_state_token.unsqueeze(1)
            last_vis_flat = rearrange(tokens[:,1:], 'b t h w d -> b (t h w) d')  # [B, L, D], L=hÂ·wÂ·t
            last_fuse, _ = self.state_cross_attn(last_vis_flat, last_ctx, last_ctx)
            
            first_fuse =  rearrange(first_fuse, 'b (t h w) d -> b t h w d', b=b, h=h, w=w)
            last_fuse =  rearrange(last_fuse, 'b (t h w) d -> b t h w d', b=b, h=h, w=w)

            tokens[:,:1] = first_fuse
            tokens[:,1:] = last_fuse
            
        tokens = rearrange(tokens, 'b t h w d -> (b t) (h w) d')

        # attn_bias shape : torch.Size([16, 64, 64])
        attn_bias = self.spatial_rel_pos_bias(h, w, device = tokens.device)

        # tokens shape : torch.Size([2, 64, 1024])
        tokens = self.enc_spatial_transformer(tokens, attn_bias = attn_bias, video_shape = video_shape)

        # tokens shape : torch.Size([1, 2, 8, 8, 1024])
        tokens = rearrange(tokens, '(b t) (h w) d -> b t h w d', b = b, h = h , w = w)
       
        

        # tokens shape : torch.Size([64, 2, 1024])
        tokens = rearrange(tokens, 'b t h w d -> (b h w) t d')

        # tokens shape : torch.Size([64, 2, 1024])
        tokens = self.enc_temporal_transformer(tokens, video_shape = video_shape)
        # tokens shape : torch.Size([1, 2, 8, 8, 1024])
        tokens = rearrange(tokens, '(b h w) t d -> b t h w d', b = b, h = h, w = w)

        # shape: [1, 1, 8, 8, 1024]
        first_tokens = tokens[:, :1]
        # shape: [1, 1, 8, 8, 1024]
        last_tokens = tokens[:, 1:]
        
        return first_tokens, last_tokens

        

    def decode(
        self,
        tokens,
        actions,
        
    ):
        # batchsize
        b = tokens.shape[0]
        # h w
        h, w = self.patch_height_width


        if tokens.ndim == 3:
            tokens = rearrange(tokens, 'b (t h w) d -> b t h w d', h = h, w = w)

        # tokens b t h w d
        # video_shape = (b, t, h, w)
        video_shape = tuple(tokens.shape[:-1])


        tokens = rearrange(tokens, 'b t h w d -> (b t) (h w) d')
        actions = rearrange(actions, 'b t h w d -> (b t) (h w) d')      # [B, code_seq_len, dim]

        # æ„å»ºç©ºé—´ç›¸å¯¹ä½ç½®åç½®ï¼ˆç”¨äº attentionï¼‰
        attn_bias = self.spatial_rel_pos_bias(h, w, device = tokens.device)

        tokens = self.dec_spatial_transformer(tokens, attn_bias = attn_bias, video_shape = video_shape, context=actions)
        

        tokens = rearrange(tokens, '(b t) (h w) d -> b t h w d', b = b, h = h , w = w) #[B, T, H, W, D]

        rest_frames_tokens = tokens

        recon_video = self.to_pixels_first_frame(rest_frames_tokens)

        # â€”â€”â€” 2. state å…³èŠ‚è§’é‡å»º â€”â€” #
        state_feat = actions.mean(dim=1)                                       # [B, dim]
        state_flat  = self.state_decoder(state_feat)                           # [B, chunksize*state_dim]
        recon_state = state_flat.view(-1, self.chunksize, self.state_dim)      # [B, chunksize, state_dim]



        # # â€”â€”â€” 2. state fk é‡å»º â€”â€” #
        state_fk_flat = self.state_fk_decoder(state_feat)   
        recon_fk_state = state_fk_flat.view(-1, self.chunksize, 14)            # # [B, chunksize, 14]

        return recon_video, recon_state, recon_fk_state
    

    def compute_state_deltas(self, target_states: torch.Tensor) -> torch.Tensor:
        """
        Computes deltas along the chunk_size (time) dimension.
        
        Args:
            target_states (torch.Tensor): Tensor of shape [B, chunk_size, state_dim]

        Returns:
            torch.Tensor: Delta tensor of same shape [B, chunk_size, state_dim]
        """
        B, T, D = target_states.shape

        # åˆå§‹åŒ–ç¬¬ä¸€å¸§ä¸º0
        zeros = torch.zeros((B, 1, D), device=target_states.device, dtype=target_states.dtype)

        # å·®åˆ†ï¼ˆä»ç¬¬2å¸§å¼€å§‹ï¼‰ï¼Œæ²¿ç€ chunk_size ç»´åº¦
        deltas = target_states[:, 1:, :] - target_states[:, :-1, :]

        # æ‹¼æ¥ï¼Œä½¿è¾“å‡º shape ä¿æŒä¸€è‡´ [B, T, D]
        delta_states = torch.cat([zeros, deltas], dim=1)

        return delta_states
    

    def forward(
        self,
        video,
        action_state = None,
        fk_result_state = None,
        step = 0,
        mask = None,
        return_recons_only = False,
        return_only_codebook_ids = False,
    ):
        
        """
        video shape: [B, C, 2, 256, 256]
        video.ndim == 5 ç°åœ¨æ˜¯å¸¦æœ‰æ—¶é—´ç»´åº¦çš„
        """
        cond_token_proj = None
        assert video.ndim in {4, 5}

        is_image = video.ndim == 4
        

        if is_image:
            video = rearrange(video, 'b c h w -> b c 1 h w')
            assert not exists(mask)
        
        """
        é‡æ–°ä» video çš„ tensor ä¸­è¯»å–ç»´åº¦ä¿¡æ¯
            b = 1                        # batch size
            c = 3                        # channel
            f = 2                        # frame count
            image_dims = [256, 256]      # list, å­˜å‚¨ height å’Œ width
            device = video.device
        """
        B, f, C,  H, W, device = *video.shape, video.device
        # print(f"b : {B}")
        if (H, W) != self.image_size:
            # 1) æŠŠ (B, C, f, H, W) -> (B*f, C, H, W)
            frames = video.permute(0, 2, 1, 3, 4).reshape(-1, C, H, W)

            frames = frames.float()  # æ˜ å°„åˆ° [0, 1]
            # 2) ç”¨ bilinear æ’å€¼ resize
            frames = F.interpolate(
                frames,
                size=self.image_size,       # (new_H, new_W)
                mode='bilinear',
                align_corners=False
            )
            # 3) å† reshape å› (B, C, f, new_H, new_W)
            video = frames.reshape(B, f, C, *self.image_size).permute(0, 2, 1, 3, 4)
       
        # ç¡®ä¿å›¾ç‰‡ç»´åº¦ä¸€è‡´
        assert (video.shape[3],video.shape[4]) == self.image_size
        # è¿™é‡Œ mask æ˜¯ none
        assert not exists(mask) or mask.shape[-1] == f

        """
        first_frame: ç¬¬ 1 å¸§         (shape: [b, c, 1, H, W])
        rest_frames: åé¢çš„æ‰€æœ‰å¸§     (shape: [b, c, f-1, H, W])
        """
        first_frame, rest_frames = video[:, :, :1], video[:, :, 1:]


        # TODO: check dim 
        if self.is_use_action_state and action_state is not None:
            # å–ç¬¬ä¸€æ­¥çš„ state
            first_state = action_state[:, 0, :]                  # [B, state_dim]
            last_state = action_state[:,-1,:]                       # [B, state_dim]
            first_state_token = self.action_state_proj(first_state)  # [B, dim]
            last_state_token = self.action_state_proj(last_state)    # [B, dim]
            state_token  = torch.concat((first_state_token, last_state_token),dim=1)  # [B, 2 * dim]
        

        """
        image_size = (256, 256)
        patch_size = (32, 32)
        ->
        num_patches_per_frame = (256 // 32, 256 // 32) = (8, 8)
        """

        # ç¬¬ä¸€å¼ å›¾è½¬ä¸º patch token embedding                                             shape: torch.Size([1, 1, 8, 8, 1024])
        first_frame_tokens = self.to_patch_emb_first_frame(first_frame)


        # å‰©ä¸‹å›¾è½¬ä¸º patch token embedding                                               shape: torch.Size([1, 1, 8, 8, 1024])
        rest_frames_tokens = self.to_patch_emb_first_frame(rest_frames)
        # å°†ç¬¬ 1 å¸§å’Œå‰©ä½™å¸§çš„ patch token æ²¿ç€ token ç»´åº¦æ‹¼æ¥èµ·æ¥ï¼Œå½¢æˆä¸€ä¸ªç»Ÿä¸€çš„ token åºåˆ—    shape: torch.Size([1, 2, 8, 8, 1024])
        tokens = torch.cat((first_frame_tokens, rest_frames_tokens), dim = 1)
        

        shape = tokens.shape
        # h : 8, w : 8
        *_, h, w, _ = shape

        """
        first_tokens shape : torch.Size([b, 1, 8, 8, 1024])
        last_tokens shape  : torch.Size([b, 1, 8, 8, 1024])
        """
        if self.is_use_action_state == True and action_state is not None:
            first_tokens, last_tokens = self.encode(tokens, state_token)
        else:
            first_tokens, last_tokens = self.encode(tokens)

        """
        pack
            pack çš„æ¨¡å¼ 'b * d'
                'b * d' è¡¨ç¤ºï¼š

                ç»´åº¦çš„ç¬¬ä¸€ä¸ªæ˜¯ b

                æœ€åä¸€ä¸ªæ˜¯ d

                ä¸­é—´çš„ç»´åº¦å…¨éƒ¨â€œæ‘Šå¹³â€æˆä¸€ä¸ªç»´åº¦ *
            è¿è¡Œå:
                first_tokens shape : torch.Size([b, 64, 1024])
                last_tokens shape  : torch.Size([b, 64, 1024])
        """
        first_tokens, first_packed_fhw_shape = pack([first_tokens], 'b * d')
        last_tokens, last_packed_fhw_shape = pack([last_tokens], 'b * d')

        vq_mask = None
        if exists(mask):
            vq_mask = self.calculate_video_token_mask(video, mask)
        self.lookup_free_quantization = False
        vq_kwargs = dict(mask = vq_mask) if not self.lookup_free_quantization else dict()

        # tokens shape : torch.Size([1, 4, 1024]) è¿™é‡Œçš„ token ä»£è¡¨éšå˜é‡
        tokens, perplexity, codebook_usage, indices = self.vq(first_tokens, last_tokens, codebook_training_only = False)
        
        # num_unique_indices : 3
        num_unique_indices = indices.unique().size(0)
      

        
        if ((step % 10 == 0 and step < 100)  or (step % 100 == 0 and step < 1000) or (step % 500 == 0 and step < 5000)) and step != 0:
            print(f"update codebook {step}")
            self.vq.replace_unused_codebooks(tokens.shape[0])

        if return_only_codebook_ids:
            return indices
        
        if math.sqrt(self.code_seq_len) % 1 == 0: # "code_seq_len should be square number"
            action_h = int(math.sqrt(self.code_seq_len))
            action_w = int(math.sqrt(self.code_seq_len))
        elif self.code_seq_len == 2:
            action_h = 2
            action_w = 1
        else:
            ## error
            print("code_seq_len should be square number or defined as 2")
            return

        # tokens shape : torch.Size([1, 1, 2, 2, 1024])
        # self.code_seq_len : 4
        # action_h : 2, action_w : 2
        tokens = rearrange(tokens, 'b (t h w) d -> b t h w d', h = action_h, w = action_w)
        # concat_tokens shape : torch.Size([1, 1, 8, 8, 1024])
        concat_tokens = first_frame_tokens.detach() # + tokens
        # recon_video shape : torch.Size([1, 3, 1, 256, 256])   å°†ç¼–ç  tokens è½¬æ¢å›è§†é¢‘å¸§çš„åƒç´ ç©ºé—´
        recon_video, recon_state, recon_fk_state = self.decode(concat_tokens, tokens)

        # returned_recon shape : torch.Size([1, 3, 256, 256])
        returned_recon = rearrange(recon_video, 'b c 1 h w -> b c h w')

        # video shape : torch.Size([1, 3, 1, 256, 256])
        video = rest_frames
        
        if return_recons_only:
            return returned_recon, recon_state
        

        """
        è¿™é‡Œæš‚æ—¶æ²¡æœ‰ mask
        """
        if exists(mask):
            # variable lengthed video / images training
            recon_loss = F.mse_loss(video, recon_video, reduction = 'none')
            recon_loss = recon_loss[repeat(mask, 'b t -> b c t', c = C)]
            recon_img_loss = recon_loss.mean()
        else:
            recon_img_loss = F.mse_loss(video, recon_video)

        if self.is_use_action_state:
            target_states = action_state[:, 1:1 + self.chunksize, :]
            if self.is_use_delta_action_state:
                target_states = self.compute_state_deltas(target_states)
            print(f"target_states shape : {target_states.shape}")
            recon_state_loss = F.mse_loss(recon_state, target_states)
        else:
            recon_state_loss = torch.tensor(0.0, device=video.device)


        if self.is_use_fk_state and fk_result_state is not None:
            target_fk = fk_result_state[:, 1:1 + self.chunksize, :]
            recon_fk_state_loss = F.mse_loss(recon_fk_state, target_fk)
        else:
            recon_fk_loss = torch.tensor(0.0, device=video.device)
        

        # recon_loss = recon_img_loss + 0.5 * recon_state_loss + 0.08 * recon_fk_state_loss
        recon_loss = recon_img_loss


        return recon_img_loss, recon_state_loss, recon_fk_state_loss, recon_loss, num_unique_indices
        

    def inference(
        self,
        video,
        step = 0,
        mask = None,
        return_only_codebook_ids=False,
        user_action_token_num=None
    ):
        
        assert video.ndim in {4, 5}

        is_image = video.ndim == 4

        if is_image:
            video = rearrange(video, 'b c h w -> b c 1 h w')
            assert not exists(mask)

        b, c, f, *image_dims, device = *video.shape, video.device

        assert tuple(image_dims) == self.image_size
        assert not exists(mask) or mask.shape[-1] == f

        first_frame, rest_frames = video[:, :, :1], video[:, :, 1:]

        first_frame_tokens = self.to_patch_emb_first_frame(first_frame)
        rest_frames_tokens = self.to_patch_emb_first_frame(rest_frames)
        tokens = torch.cat((first_frame_tokens, rest_frames_tokens), dim = 1)


        shape = tokens.shape
        *_, h, w, _ = shape

        first_tokens, last_tokens = self.encode(tokens)

        # quantize
        first_tokens, first_packed_fhw_shape = pack([first_tokens], 'b * d')
        last_tokens, last_packed_fhw_shape = pack([last_tokens], 'b * d')

        if user_action_token_num is not None:
            tokens, indices = self.vq.inference(first_tokens, last_tokens, user_action_token_num=user_action_token_num)
        else:
            tokens, indices = self.vq.inference(first_tokens, last_tokens)

        
    
        if return_only_codebook_ids:
            return indices

        if math.sqrt(self.code_seq_len) % 1 == 0: # "code_seq_len should be square number"
            action_h = int(math.sqrt(self.code_seq_len))
            action_w = int(math.sqrt(self.code_seq_len))
        elif self.code_seq_len == 2:
            action_h = 2
            action_w = 1
        else:
            print("code_seq_len should be square number or defined as 2")
            return
        

        tokens = rearrange(tokens, 'b (t h w) d -> b t h w d', h = action_h, w = action_w)
        concat_tokens = first_frame_tokens #.detach() #+ tokens
        recon_video = self.decode(concat_tokens, actions=tokens)
        returned_recon = rearrange(recon_video, 'b c 1 h w -> b c h w')
        video = rest_frames 
        
        return returned_recon

