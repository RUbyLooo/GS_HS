from math import sqrt
from random import choice
from pathlib import Path
from shutil import rmtree
import wandb

from beartype import beartype

import torch
from torch import nn
from torch.utils.data import Dataset, DataLoader, random_split
from torchvision.utils import make_grid, save_image
from torch.utils.tensorboard import SummaryWriter
import torchvision.transforms.functional as TF
from PIL import Image, ImageDraw, ImageFont
import torchvision.transforms as T
import torch.nn.functional as F
from lerobot.common.policies.egovla.laq.laq_model.optimizer import get_optimizer
from lerobot.common.policies.egovla.configuration_egovla import EgoVLAConfig

from ema_pytorch import EMA
from lerobot.common.datasets.factory import make_dataset
from lerobot.common.datasets.sampler import EpisodeAwareSampler
from lerobot.common.policies.egovla.laq.laq_model.data import ParquetTrajectoryDataset

from lerobot.configs.default import DatasetConfig
from accelerate import Accelerator, DistributedDataParallelKwargs

from einops import rearrange
from dataclasses import dataclass, field
from lerobot.configs.train import TrainPipelineConfig
from lerobot.common.optim.optimizers import AdamWConfig
from lerobot.common.optim.schedulers import (
    CosineDecayWithWarmupSchedulerConfig,
)
from lerobot.configs.policies import PreTrainedConfig
from lerobot.configs.types import FeatureType, NormalizationMode, PolicyFeature
import numpy as np
from lerobot.common.policies.egovla.laq.laq_model.piper_arm import PiperArm
from scipy.spatial.transform import Rotation as R

def exists(val):
    return val is not None
import torch.nn.functional as F

def noop(*args, **kwargs):
    pass

def cycle(dl):
    while True:
        for data in dl:
            yield data

def accum_log(log, new_logs):
    for key, new_value in new_logs.items():
        old_value = log.get(key, 0.)
        log[key] = old_value + new_value
    return log



@beartype
class LAQTrainer(nn.Module):
    def __init__(
        self,
        vae,                               # éœ€è¦è®­ç»ƒçš„ VAE æ¨¡å‹ï¼ˆå¸¦ image_size å±æ€§ï¼‰
        *,
        num_train_steps,                   # æ€»è®­ç»ƒæ­¥æ•°
        batch_size,                        # æ¯æ¬¡è®­ç»ƒçš„æ‰¹æ¬¡å¤§å°
        folder,                            # æ•°æ®æ‰€åœ¨æ–‡ä»¶å¤¹è·¯å¾„
        traj_info=None,                    # å¯é€‰ï¼šè½¨è¿¹å…ƒä¿¡æ¯ï¼ˆæœªä½¿ç”¨ï¼‰
        train_on_images=False,             # æ ‡è®°æ˜¯å¦æŒ‰å›¾åƒæ–¹å¼è®­ç»ƒï¼ˆæœªä½¿ç”¨ï¼‰
        lr=3e-4,                           # å­¦ä¹ ç‡
        grad_accum_every=1,                # æ¢¯åº¦ç´¯ç§¯æ­¥æ•°ï¼ˆå®é™… batch_size = batch_size * grad_accum_everyï¼‰
        wd=0.,                             # æƒé‡è¡°å‡
        max_grad_norm=0.5,                 # æ¢¯åº¦è£å‰ªé˜ˆå€¼ï¼ˆç”¨äºé˜²æ­¢ exploding gradientsï¼‰
        discr_max_grad_norm=None,          # åˆ¤åˆ«å™¨çš„æ¢¯åº¦è£å‰ªé˜ˆå€¼
        save_results_every=50,             # ä¿å­˜å¯è§†åŒ–ç»“æœé¢‘ç‡
        save_model_every=9998,             # ä¿å­˜æ¨¡å‹é¢‘ç‡
        input_mode="top_cam_only",         # è®­ç»ƒè¾“å…¥çš„æ¨¡å¼
        results_folder='/home/jiziheng/Music/robot/gs_scene/gs_hs/lerobot/common/policies/egovla/laq/results/0721',        # ä¿å­˜ç»“æœçš„ç›®å½•
        use_ema=True,                      # æ˜¯å¦ä½¿ç”¨ EMAï¼ˆæŒ‡æ•°æ»‘åŠ¨å¹³å‡ï¼‰VAE è¿›è¡Œæ¨ç†
        ema_update_after_step=0,           # EMA å¼€å§‹æ›´æ–°çš„æ­¥æ•°
        ema_update_every=1,                # EMA æ›´æ–°é¢‘ç‡
        accelerate_kwargs: dict = dict(),  # Accelerator é…ç½®å‚æ•°
        weights=None,                      # å¯é€‰é¢„è®­ç»ƒæ¨¡å‹æƒé‡
        offsets=None,                      # æ•°æ®åŠ è½½æ—¶å›¾åƒå¸§çš„åç§»é‡ï¼ˆtemporal offsetï¼‰
    ):
        super().__init__()
        image_size = vae.image_size

        # Accelerator åŠ é€Ÿå™¨è®¾ç½®ï¼ˆç”¨äºåˆ†å¸ƒå¼ã€æ··åˆç²¾åº¦è®­ç»ƒï¼‰
        ddp_kwargs = DistributedDataParallelKwargs(find_unused_parameters = True)
        self.accelerator = Accelerator(**accelerate_kwargs, kwargs_handlers=[ddp_kwargs])
        # ğŸ“Œ è®°å½•æ¨¡å‹å’Œè®­ç»ƒå‚æ•°
        self.vae = vae
        self.results_folder_str = results_folder
        self.lr = lr
        # ğŸ“ˆ EMA æ¨¡å‹ï¼ˆå¯é€‰ï¼‰
        self.use_ema = use_ema
        if self.is_main and use_ema:
            self.ema_vae = EMA(vae, update_after_step = ema_update_after_step, update_every = ema_update_every)
        # ğŸ§® æ­¥æ•°è¿½è¸ªå™¨
        self.register_buffer('steps', torch.Tensor([0]))

        self.num_train_steps = num_train_steps
        self.batch_size = batch_size
        self.grad_accum_every = grad_accum_every

        self.vae.discr = None # this seems to be missing

        self.input_mode = input_mode
        # â€”â€” TensorBoard writer â€”â€” 
        self.writer = SummaryWriter(log_dir=str(Path(self.results_folder_str) / 'tensorboard'))

        if exists(self.vae.discr):
            all_parameters = set(vae.parameters())
            discr_parameters = set(vae.discr.parameters())
            vae_parameters = all_parameters - discr_parameters

            self.vae_parameters = vae_parameters

            self.optim = get_optimizer(vae_parameters, lr = lr, wd = wd)
            self.discr_optim = get_optimizer(discr_parameters, lr = lr, wd = wd)
        else:
            self.vae_parameters  = set(vae.parameters())
            self.optim = get_optimizer(self.vae_parameters, lr = lr, wd = wd)

        self.max_grad_norm = max_grad_norm
        self.discr_max_grad_norm = discr_max_grad_norm

        # create dataset
        self.train_on_images = train_on_images
        
        
        # lerobot training
        # self.ds = ParquetTrajectoryDataset(folder, image_size, offset=offsets, input_mode=input_mode)


        folder = "/home/cfy/data/icra_data/data/fold_cloth"
        ds_cfg = DatasetConfig(
            repo_id="kelo234/folding",
            root=folder,
            revision=None,
            episodes=None,
            use_imagenet_stats=False,
            video_backend="cv2",
        )

        # 3. æ„é€ ä¸€ä¸ªå¸¦ä¸Šä½ é‚£ä¸¤ä¸ª property çš„ policy config
        policy_cfg = EgoVLAConfig(
            # n_obs_steps=1 å·²ç»æ˜¯é»˜è®¤ï¼Œè¡¨ç¤ºåªè¦å½“å‰å¸§
            chunk_size=50,              # state åºåˆ—é•¿åº¦
            laq_supervision=True,       # é»˜è®¤ Trueï¼Œæ‰ä¼šåŠ ä¸Šæœªæ¥ offset å¸§
            laq_temporal_offset=50,     # è¡¨ç¤ºè¿˜è¦åŠ è½½ç¬¬ 50 å¸§
        )

        # éªŒç®—ä¸€ä¸‹ä¸¤ä¸ªå±æ€§ï¼š
        print("obs deltas:", policy_cfg.observation_delta_indices)  
        # => [0, 50]

        print("action deltas:", policy_cfg.action_delta_indices[:5], "...", len(policy_cfg.action_delta_indices))
        # => [0, 1, 2, 3, 4] ... 50

        # 4. pretrained config åªè¦ç©ºå£³
        # pretrained_cfg = PreTrainedConfig()

        # 5. æ‰“åŒ…æˆ pipeline config
        pipeline_cfg = TrainPipelineConfig(
            dataset=ds_cfg,
            policy=policy_cfg,
        )

        # 6. è°ƒç”¨ make_datasetï¼Œç›´æ¥å¾—åˆ°ä½ æƒ³è¦çš„ dataset
        self.ds = make_dataset(pipeline_cfg)
        sampler = EpisodeAwareSampler(
            self.ds.episode_data_index,
            drop_n_last_frames=True,
            shuffle=True,
        )
        self.dl = DataLoader(
            self.ds,
            num_workers=8,
            batch_size=batch_size,
            shuffle=False,
            sampler=sampler,
            pin_memory="cuda",
            drop_last=False,
        )
        
            
        # âš ï¸ è¿™è¡¨ç¤ºè®­ç»ƒé›†å’ŒéªŒè¯é›†ä½¿ç”¨å®Œå…¨ç›¸åŒæ•°æ®
        self.valid_ds = self.ds

        """
        âœ… ç”¨äºè®­ç»ƒé›†çš„ DataLoader
            å‚æ•°è¯´æ˜ï¼š
                shuffle=True              :                   æ‰“ä¹±æ•°æ®ï¼Œæœ‰åŠ©äºæ³›åŒ–
                num_workers=1             :                   ä»…å¼€ 1 ä¸ªè¿›ç¨‹åŠ è½½æ•°æ®
                pin_memory=True           :                   åŠ é€Ÿå°†æ•°æ®ä¼ è¾“åˆ° GPU
                prefetch_factor=2         :                   æ¯ä¸ª worker é¢„å–ä¸¤ä¸ª batch
        """

        """
        âœ… ç”¨äºéªŒè¯é›†çš„ DataLoader
            å‚æ•°è¯´æ˜ï¼š
                shuffle=False             :                   éªŒè¯æ•°æ®ä¿æŒé¡ºåº
                num_workers=4             :                   éªŒè¯é›†é€šå¸¸å¯ä»¥å¼€å¾—æ›´é«˜ï¼Œå› ä¸ºä¸ç”¨åå‘ä¼ æ’­
        """
        self.valid_dl = DataLoader(
            self.valid_ds,
            batch_size = batch_size,
            num_workers = 16,
            drop_last=True,
            shuffle=False,
            sampler=sampler
        )

        # âš¡ï¸ ä½¿ç”¨ HuggingFace Accelerate è¿›è¡Œåˆ†å¸ƒå¼å‡†å¤‡
        if exists(self.vae.discr):
            (
                self.vae,
                self.optim,
                self.discr_optim,
                self.dl
            ) = self.accelerator.prepare(
                self.vae,
                self.optim,
                self.discr_optim,
                self.dl
            )
        else:
            (
                self.vae,
                self.optim,
                self.dl
            ) = self.accelerator.prepare(
                self.vae,
                self.optim,
                self.dl
            )

        # ğŸ” æ„é€  DataLoader çš„æ— é™å¾ªç¯è¿­ä»£å™¨
        self.dl_iter = cycle(self.dl)
        # self.dl_iter = cycle(self.dl)
        self.valid_dl_iter = cycle(self.valid_dl)

        self.save_model_every = save_model_every
        self.save_results_every = save_results_every


        self.results_folder = Path(results_folder)


        self.results_folder.mkdir(parents = True, exist_ok = True)


        # å…³èŠ‚è§’é™åˆ¶å¹…åº¦
        self.moving_limits = np.array([
            (0.0, 1.0),
            (0.0, 1.0),
            (-2.618, 2.618),
            (0, 3.14),
            (-2.697, 0),
            (-1.832, 1.832),
            (-1.22, 1.22),
            (-3.14, 3.14),
            (0, 1),
            (-2.618, 2.618),
            (0, 3.14),
            (-2.697, 0),
            (-1.832, 1.832),
            (-1.22, 1.22),
            (-3.14, 3.14),
            (0, 1)
        ])

        self.piper_arm = PiperArm()

    def save(self, path):
        if not self.accelerator.is_local_main_process:
            return

        if exists(self.vae.discr):
            pkg = dict(
                model = self.accelerator.get_state_dict(self.vae),
                optim = self.optim.state_dict(),
                discr_optim = self.discr_optim.state_dict(),
                steps = self.steps.item()
            )
        else:
            pkg = dict(
                model=self.accelerator.get_state_dict(self.vae),
                optim=self.optim.state_dict(),
                steps=self.steps.item()
            )

        # Save DataLoader state
        pkg['dl_iter_state'] = self.get_dl_state(self.dl_iter)

        torch.save(pkg, path)

    def load(self, path):
        path = Path(path)
        assert path.exists()
        pkg = torch.load(path)

        vae = self.accelerator.unwrap_model(self.vae)
        vae.load_state_dict(pkg['model'])

        self.optim.load_state_dict(pkg['optim'])
        if exists(self.vae.discr):
            self.discr_optim.load_state_dict(pkg['discr_optim'])


    def cal_fk_result(self, action_state):
        """
        action_state: [B, T, 16]
        output: [B, T, 14] - [left_xyz + left_wxyz + right_xyz + right_wxyz]
        """

        B, T, _ = action_state.shape
        action_np = action_state.cpu().numpy()  # è½¬ numpyï¼Œæ–¹ä¾¿é€ä¸ªå¤„ç†

        result = []

        for b in range(B):
            traj = []
            for t in range(T):
                # æå–å·¦è‡‚å…³èŠ‚è§’ï¼ˆå¼§åº¦ï¼‰
                left_joints = action_np[b, t, 2:8]  # [6]
                # æå–å³è‡‚å…³èŠ‚è§’ï¼ˆå¼§åº¦ï¼‰
                right_joints = action_np[b, t, 9:15]  # [6]

                # FK å·¦è‡‚
                T_left = self.piper_arm.forward_kinematics(left_joints)  # [4, 4]
                pos_left = T_left[:3, 3]  # [x, y, z]
                rot_left = R.from_matrix(T_left[:3, :3]).as_quat()  # [x, y, z, w]

                # FK å³è‡‚
                T_right = self.piper_arm.forward_kinematics(right_joints)
                pos_right = T_right[:3, 3]
                rot_right = R.from_matrix(T_right[:3, :3]).as_quat()

                # å°†å››å…ƒæ•°å˜ä¸º wxyz é¡ºåº
                quat_left = np.concatenate(([rot_left[3]], rot_left[:3]))   # [w, x, y, z]
                quat_right = np.concatenate(([rot_right[3]], rot_right[:3]))

                # æ‹¼æ¥å·¦å³è‡‚çš„æœ«ç«¯ä½å§¿
                frame_pose = np.concatenate([pos_left, quat_left, pos_right, quat_right])  # [14]
                traj.append(frame_pose)

            result.append(traj)

        result = torch.tensor(result, dtype=action_state.dtype, device=action_state.device)  # [B, T, 14]
        return result




    def print(self, msg):
        self.accelerator.print(msg)

    @property
    def device(self):
        return self.accelerator.device

    @property
    def is_distributed(self):
        return not (self.accelerator.distributed_type == DistributedType.NO and self.accelerator.num_processes == 1)

    @property
    def is_main(self):
        return self.accelerator.is_main_process

    @property
    def is_local_main(self):
        return self.accelerator.is_local_main_process

    def train_step(self):
        # æ¨¡å‹/æ•°æ®å°†è¢«ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡
        device = self.device
        # ä» Tensor ä¸­å–å‡ºå½“å‰è®­ç»ƒæ­¥æ•°
        steps = int(self.steps.item())
        # å°† VAE ç½®äºè®­ç»ƒæ¨¡å¼ï¼ˆå¯ç”¨ dropout ç­‰ï¼‰
        self.vae.train()

        # logs

        logs = {}
        # print(f"train_step !!!")

        # update vae (generator)

        for _ in range(self.grad_accum_every):
            # è®­ç»ƒæ•°æ®è¿­ä»£å™¨
            if self.input_mode == "top_cam_only":
                img = next(self.dl_iter)
                img = img.to(device)
                # å‰å‘ä¼ æ’­ï¼Œè®¡ç®—é‡å»ºæŸå¤±å’Œç æœ¬ä¸­ç”¨åˆ°çš„å”¯ä¸€ token æ•°
                recon_img_loss, recon_state_loss, loss, num_unique_indices = self.vae(
                    img,
                    step=steps,
                )
            elif self.input_mode == "top_cam_state":
                # action_state : [B, 51, 16]
                batch = next(self.dl_iter)
                # print("...")
                # img, action_state = next(self.dl_iter)
                img = batch["observation.images.top"]
                action_state = batch["action"]

                fk_result_state = self.cal_fk_result(action_state)       # [B, chunk_size, 14]

                fk_min_vals = fk_result_state.amin(dim=(0, 1), keepdim=True)  # shape [1, 1, 14]
                fk_max_vals = fk_result_state.amax(dim=(0, 1), keepdim=True)  # shape [1, 1, 14]

                # é¿å…é™¤ä»¥ 0
                fk_range_vals = fk_max_vals - fk_min_vals
                fk_range_vals[fk_range_vals == 0] = 1e-6

                fk_normalized = (fk_result_state - fk_min_vals) / fk_range_vals  # shape [B, T, 14]


                # å‡è®¾ action_state shape: [batch, chunk_size, 16]
                # å‡è®¾ self.moving_limits æ˜¯ numpy æ•°ç»„ shape: [16, 2]
                min_vals = torch.tensor(self.moving_limits[:, 0], dtype=action_state.dtype, device=action_state.device)
                max_vals = torch.tensor(self.moving_limits[:, 1], dtype=action_state.dtype, device=action_state.device)

                # Reshape ä¸º [1, 1, 16] ä»¥ä¾¿å¹¿æ’­åˆ°æ•´ä¸ª batch å’Œ chunk
                min_vals = min_vals.view(1, 1, -1)
                max_vals = max_vals.view(1, 1, -1)

                # é¿å…é™¤ä»¥ 0 çš„æƒ…å†µï¼ˆå¦‚æœ max==minï¼‰
                range_vals = max_vals - min_vals
                range_vals[range_vals == 0] = 1e-6  # é¿å…é™¤ä»¥é›¶
                action_state = (action_state - min_vals) / range_vals
                img = img.to(device)
                action_state.to(device)
                # å‰å‘ä¼ æ’­ï¼Œè®¡ç®—é‡å»ºæŸå¤±å’Œç æœ¬ä¸­ç”¨åˆ°çš„å”¯ä¸€ token æ•°
                recon_img_loss, recon_state_loss, recon_fk_state_loss, loss, num_unique_indices = self.vae(
                    img,
                    action_state=action_state,
                    fk_result_state=fk_normalized,
                    step=steps,
                )
            else:
                raise ValueError("you must provided a correct input mode. ")
            
            
            # åå‘ä¼ æ’­æ¢¯åº¦ï¼Œè€ƒè™‘æ¢¯åº¦ç´¯ç§¯
            self.accelerator.backward(loss / self.grad_accum_every)

            accum_log(logs, {'loss':             loss.item() / self.grad_accum_every})
            accum_log(logs, {'recon_img_loss':   recon_img_loss.item() / self.grad_accum_every})
            accum_log(logs, {'recon_state_loss': recon_state_loss.item() / self.grad_accum_every})
            accum_log(logs, {'recon_fk_state_loss': recon_fk_state_loss.item() / self.grad_accum_every})
            accum_log(logs, {'num_unique_indices': num_unique_indices})

              # æ˜¾å¼å†™å…¥å„é¡¹ metric åˆ° TensorBoard
            
        # æ¢¯åº¦è£å‰ª & ä¼˜åŒ–å™¨æ›´æ–°
        if exists(self.max_grad_norm):
            self.accelerator.clip_grad_norm_(self.vae.parameters(), self.max_grad_norm)

        self.optim.step()
        self.optim.zero_grad()

        step = int(self.steps.item())
        self.writer.add_scalar('loss',               logs['loss'],               step)
        self.writer.add_scalar('recon_img_loss',     logs['recon_img_loss'],     step)
        self.writer.add_scalar('recon_state_loss',   logs['recon_state_loss'],   step)
        self.writer.add_scalar('recon_fk_state_loss',   logs['recon_fk_state_loss'],   step)
        self.writer.add_scalar('num_unique_indices', logs['num_unique_indices'], step)


        # if self.is_main:  # Ensure only the main process logs in a distributed setting
        #     wandb.log(logs)

        if self.is_main and self.use_ema:
            self.ema_vae.update()

        if self.is_main and not (steps % self.save_results_every):
            unwrapped_vae = self.accelerator.unwrap_model(self.vae)
            vaes_to_evaluate = ((unwrapped_vae, str(steps)),)

            if self.use_ema:
                vaes_to_evaluate = ((self.ema_vae.ema_model, f'{steps}.ema'),) + vaes_to_evaluate

            for model, filename in vaes_to_evaluate:
                model.eval()

                if self.input_mode == "top_cam_only":
                    valid_data = next(self.valid_dl_iter)
                elif self.input_mode == "top_cam_state":
                    val_batch = next(self.valid_dl_iter)
                    valid_data = val_batch["observation.images.top"]
                    valid_action_data = val_batch["action"]
                else:
                    raise ValueError("you must provided a correct input mode. ")


                valid_data = valid_data.to(device)
                valid_action_data = valid_action_data.to(device)

                recons_img, recons_state = model(valid_data, valid_action_data, return_recons_only = True)


                if self.train_on_images:
                    imgs_and_recons = torch.stack((valid_data, recons_img), dim = 0)
                    # imgs_and_recons = torch.stack((valid_data, recons), dim = 0)
                    imgs_and_recons = rearrange(imgs_and_recons, 'r b ... -> (b r) ...')

                    imgs_and_recons = imgs_and_recons.detach().cpu().float().clamp(0., 1.)
                    grid = make_grid(imgs_and_recons, nrow = 2, normalize = True, value_range = (0, 1))

                    logs['reconstructions'] = grid
                    save_image(grid, str(self.results_folder / f'{filename}.png'))
                else:
                    # imgs_and_recons = torch.stack((valid_data[:,:,0],valid_data[:,:,-1], recons, recons+valid_data[:,:,0]), dim = 0)
                    frame0 = valid_data.permute(0,2,1,3,4)[:, :, 0]      # [B, C, H, W]
                    frameN = valid_data.permute(0,2,1,3,4)[:, :, -1]     # [B, C, H, W]
                    _, _, H_rec, W_rec = recons_img.shape
                    # å¦‚æœåŸå§‹å¸§å°ºå¯¸ != é‡æ„å¸§å°ºå¯¸ï¼Œå°±æ’å€¼ resize
                    if frame0.shape[-2:] != (H_rec, W_rec):
                        frame0 = F.interpolate(frame0, size=(H_rec, W_rec),
                                            mode='bilinear', align_corners=False)
                        frameN = F.interpolate(frameN, size=(H_rec, W_rec),
                                            mode='bilinear', align_corners=False)

                    imgs_and_recons = torch.stack((frame0,frameN, recons_img), dim = 0)
                    # imgs_and_recons = torch.stack((valid_data, recons), dim = 0)
                    imgs_and_recons = rearrange(imgs_and_recons, 'r b ... -> (b r) ...')

                    imgs_and_recons = imgs_and_recons.detach().cpu().float().clamp(0., 1.)
                    grid = make_grid(imgs_and_recons, nrow = 3, normalize = False, value_range = (0, 1))

                    pil = TF.to_pil_image(grid)  
                    draw = ImageDraw.Draw(pil)
                    font = ImageFont.load_default()

                    B = valid_action_data.shape[0]
                    for i in range(B):
                        gt = valid_action_data[i].cpu().numpy().tolist()
                        rec = recons_state[i].cpu().detach().numpy().tolist()
                        # æ¯ä¸ªæ ·æœ¬å ç”¨ 3 å¼ å›¾ï¼Œå›¾å®½ = W_recï¼Œå›¾é«˜ = H_rec
                        W_rec, H_rec = pil.size
                        per_w = W_rec // 3
                        # åœ¨ç¬¬ i ä¸ªæ ·æœ¬è¡Œçš„æœ€ä¸Šæ–¹ã€æ¯å¼ å­å›¾å·¦ä¸Šè§’å†™
                        text = f"GT: {gt}\nREC: {rec}"
                        x = (i*3)*per_w + 5  # ç¬¬ i æ ·æœ¬ç¬¬ 1 å­å›¾å·¦è¾¹èµ·ç‚¹
                        y = 5
                        draw.text((x, y), text, font=font, fill=(255,255,255))


                    # â€”â€” 3. è½¬å› tensorï¼Œå†™å…¥ logs â€”â€” 
                    grid_annot = TF.to_tensor(pil).clamp(0,1)
                    logs['reconstructions'] = grid_annot

                    save_image(grid_annot, str(self.results_folder / f'{filename}.png'))

            self.print(f'{steps}: saving to {str(self.results_folder)}')

        if self.is_main and not (steps % self.save_model_every):
            state_dict = self.vae.state_dict()
            model_path = str(self.results_folder / f'vae.{steps}.pt')
            torch.save(state_dict, model_path)

            if self.use_ema:
                ema_state_dict = self.ema_vae.state_dict()
                model_path = str(self.results_folder / f'vae.{steps}.ema.pt')
                torch.save(ema_state_dict, model_path)

            self.print(f'{steps}: saving model to {str(self.results_folder)}')

        self.steps += 1
        return logs

    def train(self, log_fn = noop):
        device = next(self.vae.parameters()).device
        # if self.accelerator.is_main_process:
        #     wandb.init(project='phenaki_cnn',name=self.results_folder_str.split('/')[-1], config={
        #         "learning_rate": self.lr,
        #         "batch_size": self.batch_size,
        #         "num_train_steps": self.num_train_steps,
        #     })

        while self.steps < self.num_train_steps:
            logs = self.train_step()
            log_fn(logs)

        self.print('training complete')
        if self.accelerator.is_main_process:
            self.writer.close()
        #     wandb.finish()  
